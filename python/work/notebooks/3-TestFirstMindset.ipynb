{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4a5d2c",
   "metadata": {},
   "source": [
    "## Adopting a Test-First Mindset\n",
    "By approaching your generative AI project with a test-first mindset, you can ensure you are thinking about desired outcomes from the outset of your application development. In this section, we will explore some simple ways to consider the context of your data and define some ideal question-answer pairs to use as a testing guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daddef87",
   "metadata": {},
   "source": [
    "### Step 1 - Inspect a sample patient encounter\n",
    "Everything in a Retrieval-Augmented Generation (RAG) pipeline begins with understanding the raw data. Skimming a concrete record grounds the questions you’ll create next and reminds you where answers actually live (structured fields and unstructured notes).\n",
    "\n",
    "Let's load one sample encounter that we can build a test case around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 – Load a sample encounter\n",
    "\n",
    "# Required imports\n",
    "import os, pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "username = 'SuperUser'\n",
    "password = 'SYS'\n",
    "hostname = 'localhost'\n",
    "port = 1972\n",
    "namespace = 'IRISAPP'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "# Retrieve sample encounter 254\n",
    "with engine.connect() as conn:\n",
    "    with conn.begin():\n",
    "        sql = text(\"\"\"\n",
    "            SELECT Encounter_ID, Description, Clinical_Notes, DESCRIPTION_OBSERVATIONS FROM GenAI.encounters WHERE Encounter_ID='254'\n",
    "        \"\"\")\n",
    "        results = conn.execute(sql).fetchall()\n",
    "\n",
    "# Display results\n",
    "df = pd.DataFrame(results)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870be687",
   "metadata": {},
   "source": [
    "### Step 2 - Create two realistic questions\n",
    "RAG systems succeed or fail on the questions people really ask. Crafting them yourself ensures your future tests reflect authentic clinical or analytic needs.\n",
    "\n",
    "Pretend you’re a clinician or researcher looking at the clinical notes from Encounter 254 above. Add exactly two natural-language questions to the qa_pairs list. Aim for specificity (“Which comorbidities existed before this admission?”) instead of broad asks (“Tell me about the patient”). Add the questions in the placeholder code below, then run the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 – Add your two questions below\n",
    "qa_pairs = [\n",
    "    # Example:\n",
    "    # {\"question\": \"What comorbidities does this patient have prior to this encounter?\"},\n",
    "    # {\"question\": \"Which medications were administered during this visit?\"}\n",
    "]\n",
    "\n",
    "qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664577f",
   "metadata": {},
   "source": [
    "### Step 3 - Define one \"gold answer\"\n",
    "A single high-quality Q&A pair acts as a “canary” -- if later tweaks break this exact fact retrieval, you’ll see it immediately. Ground truth before you code.\n",
    "\n",
    "Pick one of your two questions and paste the precise answer text straight from the encounter. Fill these into the `gold_answer` dictionary.\n",
    "\n",
    "1. `question` – copy the question text.\n",
    "2. `expected_answer` – paste the exact snippet (≤ 3 sentences) from the encounter note/fields.\n",
    "3. `source` – list the encounter ID that proves the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824204b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 – Fill in your gold answer\n",
    "gold_answer = {\n",
    "    \"question\": \"\",           # copy one question here\n",
    "    \"expected_answer\": \"\",    # paste the exact text snippet here\n",
    "    \"sources\": \"\"             # e.g., \"encounter 254\"\n",
    "}\n",
    "\n",
    "gold_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f1f3d",
   "metadata": {},
   "source": [
    "### Step 4 - Run baseline retrieval + generation\n",
    "Execute the stubbed retrieve_and_generate() function for your questions. You’ll see the model’s answer, the chunks it cited, and an automatic green ✅ / red ❌ check against your gold answer.\n",
    "\n",
    "This is your baseline. All future tuning (different indexes, prompt tweaks, agents) must keep the green light on. Seeing a failure now tells you where the pipeline needs help later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS GEN-AI CREATED; THIS MAY NOT WORK BUT KEEPING HERE IN CASE IT TRIGGERS ANY IDEAS.\n",
    "\n",
    "# Step 4 – Run baseline retrieval and generation (stub)\n",
    "def retrieve_and_generate(question):\n",
    "    \"\"\"    TODO: Implement this with your own retrieval + generation pipeline.\n",
    "    It should return a dict: {'answer': str, 'sources': [str]}\n",
    "    \"\"\"\n",
    "    return {\"answer\": \"placeholder answer\", \"sources\": [\"E123|note_chunk_7\"]}\n",
    "\n",
    "# Show answers for both questions\n",
    "for q in qa_pairs:\n",
    "    text = q[\"question\"] if isinstance(q, dict) else q\n",
    "    result = retrieve_and_generate(text)\n",
    "    print(\"Q:\", text)\n",
    "    print(\"A:\", result['answer'])\n",
    "    print(\"Sources:\", result['sources'])\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Simple pass/fail for the gold answer\n",
    "if gold_answer[\"question\"]:\n",
    "    res = retrieve_and_generate(gold_answer[\"question\"])\n",
    "    hit = any(src in res[\"sources\"] for src in gold_answer[\"sources\"])\n",
    "    status = \"✅ PASS\" if hit else \"❌ FAIL\"\n",
    "    print(status, \"- expected source(s)\", \"found.\" if hit else \"NOT found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fb62a",
   "metadata": {},
   "source": [
    "### Step 5 - Save your tiny test set\n",
    "From this moment on, every notebook you touch can import this test file to ensure retrieval remains correct. It’s your regression guardrail for the rest of the workshop -- and a template you can copy into your own projects.\n",
    "\n",
    "Run the module below to write `tests/tiny_test.json`, which stores your two questions and one gold answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS GEN-AI CREATED; THIS MAY NOT WORK BUT KEEPING HERE IN CASE IT TRIGGERS ANY IDEAS.\n",
    "\n",
    "# Step 5 – Save tiny_test.json\n",
    "import json, os, pathlib, datetime\n",
    "\n",
    "tiny_test = {\n",
    "    \"created\": datetime.datetime.utcnow().isoformat(),\n",
    "    \"qa_pairs\": qa_pairs,\n",
    "    \"gold_answer\": gold_answer\n",
    "}\n",
    "\n",
    "os.makedirs(\"tests\", exist_ok=True)\n",
    "path = pathlib.Path(\"tests/tiny_test.json\")\n",
    "path.write_text(json.dumps(tiny_test, indent=2))\n",
    "print(f\"Saved {path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581499bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
