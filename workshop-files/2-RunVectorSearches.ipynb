{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Run Some Vector Searches\n",
    "Now that you have created and run a simple Streamlit application that essentially acts as a basic LLM, let's load some data into InterSystems IRIS and run vector searches on this data. We will later connect this vector storage to the Streamlit application so that the chat app can utilize this data.\n",
    "\n",
    "We will use the *FastEmbed* embeddings model to vectorize our data, and we'll use Langchain to load and interact with the data. Langchain provides several advantages in building a RAG application, including streamlining the retrieval process, adding conversation history, enabling guardrails to keep your application within its intended usage, and more. We will implement these features of the chat later in the workshop.\n",
    "\n",
    "Throughout the code snippets in this workshop, you may see lines of code commented out, which can be used in later iterations when you take the code home. For example, you could use the *OpenAIEmbeddings* model to create your embeddings. This requires an OpenAI API key. For this workshop, InterSystems has provided a short-term OpenAI API key that is already configured in the environment variables. The key is also used for the base LLM that the chat application uses.\n",
    "\n",
    "The following block of code is used to manage environment variables, specifically for loading and setting the OpenAI API key. It begins by importing necessary modules for operating system interactions and secure password input. Run the block below to load these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next block imports a variety of libraries and modules for completing advanced language processing tasks. These include handling and storing documents, loading textual and JSON data, splitting text based on character count, and utilizing embeddings from OpenAI, Hugging Face, and potentially faster embedding methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader, JSONLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "from langchain_iris import IRISVector\n",
    "from sqlalchemy import create_engine, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Case Reports data\n",
    "Before loading this data into InterSystems IRIS, let's take a quick look at it. We can use a Pandas DataFrame to easily format and view the data, which currently exists within a `json lines` file, which is a file format where each line is a complete JSON object, separated by new lines. This format is particularly useful for handling large datasets or streams of data, because it allows for reading, writing, and processing one line (or one JSON object) at a time, rather than needing to load an entire file into memory at once.\n",
    "\n",
    "Run the snippet below to create a DataFrame and view the first 10 case reports in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSONL file into DataFrame\n",
    "file_path = './data/healthcare/augmented_notes_100.jsonl'\n",
    "df_cases = pd.read_json(file_path, lines=True)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_cases.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the case reports into InterSystems IRIS via Langchain\n",
    "Next, we will set up the process for loading, splitting, and preparing to embed text documents from the data set of 100 case reports.\n",
    "\n",
    "The first step is to initialize a *JSONLoader* to load documents from a specified file. The line *json_lines=True* below specifies that we are loading files from a *json_lines* file.\n",
    "\n",
    "After loading the data, the text can be split into smaller chunks to facilitate more efficient processing and embedding. Here, we use a chunk size of 2,500 characters with an overlap of 100 characters. With these settings, each case report will be a single chunk.\n",
    "\n",
    "Chunking the text helps in managing large documents by breaking them into smaller, more manageable pieces, which can be individually embedded into vector format. The overlap ensures that important contextual information is preserved across chunks, enhancing the quality of the resulting embeddings. We will further chunk this data set later in this section.\n",
    "\n",
    "Run the block of code below to prepare these chunks. The output will indicate how many chunks were created; in this case, you should have 100 chunks, since the chunk size is large enough to fit each case report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data from json lines file\n",
    "loader = JSONLoader(\n",
    "    file_path='./data/healthcare/augmented_notes_100.jsonl',\n",
    "    jq_schema='.note',\n",
    "    json_lines=True\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Got {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With chunks created, let's prepare to load the data into InterSystems IRIS. Run the following block to create and print the connection string that will be used to connect to InterSystems IRIS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = '_SYSTEM'\n",
    "password = 'sys'\n",
    "hostname = 'localhost'\n",
    "port = 1972\n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "print(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's initialize a database in InterSystems IRIS, which you will populate with the case reports. \n",
    "\n",
    "This setup is essential for applications involving search and retrieval of information where the semantic content of the documents is more important than their keyword content. The vector store uses embeddings to perform similarity searches, offering significant advantages over traditional search methods by understanding the context and meaning embedded within the text.\n",
    "\n",
    "Run the block below to load your data and embeddings into InterSystems IRIS. Note that we are using the *FastEmbed* model to create our embeddings, but your application could use a variety of different embeddings models.\n",
    "\n",
    "This may take a few moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "\n",
    "COLLECTION_NAME = \"case_reports\"\n",
    "db = IRISVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that there are 100 documents in your InterSystems IRIS vector storage by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of docs in vector store: {len(db.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out vector search \n",
    "\n",
    "Now that the case reports are loaded, embedded, and stored in the vector database, you can try running a vector search. In the code block below, we will use the search phrase \"Have any children presented with knee injuries?\" to retrieve similar vectors from our storage.\n",
    "\n",
    "The second line in the block returns the documents along with their similarity scores, which quantify how similar each document is to the query.\n",
    "\n",
    "What is this vector search really doing? Recall earlier that we chose the *FastEmbed* model to create our embeddings. When you provide a search query in this module, an embedding is created for your query using the same embeddings model as the one we used to embed our data. Then, from the vector storage in InterSystems IRIS, the `similarity_search_with_score` function provided by Langchain is finding the most semantically similar results to the search query you provided.\n",
    "\n",
    "NOTE: Lower similarity scores indicate greater similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Have any children presented with knee injuries?\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to print the returned documents along with their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will enter a search phrase of your choice and perform a similarity search on that phrase.\n",
    "\n",
    "Set the *content* variable to a word or phrase of your choice; you can reference keywords from some of the companies you saw in the data set when initially browsing it in Step 1, if you'd like. Or you can choose any phrase you wish. Then, run the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"Upper respiratory illness\"\n",
    "docs_with_score = db.similarity_search_with_score(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the block below to print the most similar results. Observe the similarity scores, keeping in mind that the closer to 0 it is, the closer in similarity the document is to your search phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with chunk sizes\n",
    "Next, let's try experimenting with varied chunk sizes. Run the block below, which is the same as your previous iteration for loading and chunking, but this time uses a chunk size of 400 instead of 2,500. Observe the change in the number of chunks that were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data from json lines file\n",
    "loader = JSONLoader(\n",
    "    file_path='./data/healthcare/augmented_notes_100.jsonl',\n",
    "    jq_schema='.note',\n",
    "    json_lines=True\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Got {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load this newly chunked data into a separate data store within InterSystems IRIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "\n",
    "COLLECTION_NAME = \"case_reports-chunked\"\n",
    "dbchunked = IRISVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm how many documents are loaded into this second data structure in InterSystems IRIS by running the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of docs in vector store: {len(dbchunked.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try another vector search against the more finely-chunked data. Run the module below, optionally replacing your search term with the one you chose earlier. Whichever term you choose, observe how the results and similarity scores may differ slightly from before, when larger chunks were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Have any children presented with knee injuries?\"\n",
    "docs_with_score = dbchunked.similarity_search_with_score(query)\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare retrieval mechanism for RAG app\n",
    "Now that we have experimented with vector searches and chunk sizes, let's set up a retriever for our database. A retriever is an essential component in information retrieval systems, as it allows us to fetch relevant documents based on a query efficiently. By converting the database into a retriever, we enhance our ability to interact with the data, enabling more advanced search and retrieval operations.\n",
    "\n",
    "The `as_retriever()` method transforms the database into a retriever object. This object can then be used to perform various retrieval tasks, making it a versatile tool for working with our embedded documents.\n",
    "\n",
    "Run the block below to create the retriever and print it to confirm its setup. We'll use the more finely chunked data, consisting of 764 chunks, for this retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = dbchunked.as_retriever()\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
