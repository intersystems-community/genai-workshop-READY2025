{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data and Running Vector Searches\n",
    "Now that you have created and run a simple Streamlit application that essentially acts as a basic LLM, let's load some data into InterSystems IRIS and run vector searches on this data. We will later connect this vector storage to the Streamlit application so that the chat app can utilize this data.\n",
    "\n",
    "We will use the *FastEmbed* embeddings model to vectorize our data, and we'll use Langchain to load and interact with the data. Langchain provides several advantages in building a RAG application, including streamlining the retrieval process, adding conversation history, enabling guardrails to keep your application within its intended usage, and more. We will implement these features of the chat later in the workshop.\n",
    "\n",
    "The following block of code is used to manage environment variables, specifically for loading and setting the OpenAI API key. It begins by importing necessary modules for operating system interactions and secure password input. Run the block below to load these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next block imports a variety of libraries and modules for completing advanced language processing tasks. These include handling and storing documents, loading textual and JSON data, splitting text based on character count, and utilizing embeddings from OpenAI, Hugging Face, and potentially faster embedding methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader, JSONLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "from langchain_iris import IRISVector\n",
    "from sqlalchemy import create_engine, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Case Reports data\n",
    "Before loading this data into InterSystems IRIS, let's take a quick look at it. We can use a Pandas DataFrame to easily format and view the data, which currently exists within a `json lines` file, which is a file format where each line is a complete JSON object, separated by new lines. This format is particularly useful for handling large datasets or streams of data, because it allows for reading, writing, and processing one line (or one JSON object) at a time, rather than needing to load an entire file into memory at once.\n",
    "\n",
    "Run the snippet below to create a DataFrame and view the first 10 case reports in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load JSONL file into DataFrame\n",
    "file_path = './data/healthcare/augmented_notes_100.jsonl'\n",
    "df_cases = pd.read_json(file_path, lines=True)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_cases.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the case reports into InterSystems IRIS via Langchain\n",
    "Now you'll load, split, and prepare a set of 100 case reports for embedding.\n",
    "\n",
    "1. Load the Data â€” Use a JSONLoader to read the file, specifying json_lines=True to handle the JSON Lines format.\n",
    "\n",
    "2. Split into Chunks â€” After loading, split the text into chunks of 2,500 characters with a 100-character overlap. This ensures each case report becomes one chunk while preserving important context across boundaries.\n",
    "\n",
    "3. Why Chunking Matters â€” Chunking makes large documents easier to manage and embed. The overlap improves embedding quality by retaining contextual continuity between chunks.\n",
    "\n",
    "Later in this section, youâ€™ll apply further chunking.\n",
    "\n",
    "Run the code block below to generate the chunks. You should see 100 chunks as output, since each case report fits within the defined size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data from json lines file\n",
    "loader = JSONLoader(\n",
    "    file_path='./data/healthcare/augmented_notes_100.jsonl',\n",
    "    jq_schema='.note',\n",
    "    json_lines=True\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Got {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With chunks created, let's prepare to load the data into InterSystems IRIS. Run the following block to create and print the connection string that will be used to connect to InterSystems IRIS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = '_SYSTEM'\n",
    "password = 'SYS'\n",
    "hostname = 'IRIS'\n",
    "port = 1972\n",
    "namespace = 'IRISAPP'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "print(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's initialize a database in InterSystems IRIS, which you will populate with the case reports. \n",
    "\n",
    "This setup is essential for applications involving search and retrieval of information where the semantic content of the documents is more important than their keyword content. The vector store uses embeddings to perform similarity searches, offering significant advantages over traditional search methods by understanding the context and meaning embedded within the text.\n",
    "\n",
    "Run the block below to load your data and embeddings into InterSystems IRIS. Note that we are using the *FastEmbed* model to create our embeddings, but your application could use a variety of different embeddings models.\n",
    "\n",
    "This may take a few moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = FastEmbedEmbeddings()\n",
    "# embeddings = OpenAIEmbeddings() # Only use OpenAIEmbeddings here if issues with FastEmbed\n",
    "\n",
    "COLLECTION_NAME = \"case_reports\"\n",
    "db = IRISVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "print(\"Embeddings generation for case reports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that there are 100 documents in your InterSystems IRIS vector storage by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of docs in vector store: {len(db.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out vector search\n",
    "With the case reports now embedded and stored, you can run a vector search. In the example below, we use the query:\n",
    "\n",
    "- *\"Have any children presented with knee injuries?\"*\n",
    "\n",
    "The code returns matching documents and their similarity scores, which reflect how closely each result matches the query.\n",
    "\n",
    "**How It Works:**\n",
    "- The query is converted into an embedding using the same FastEmbed model used for the case reports.\n",
    "- Langchain's `similarity_search_with_score` function searches the IRIS vector store for the most semantically similar results.\n",
    "\n",
    "ðŸ”Ž **Note:** Lower scores = higher similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Have any patients presented with knee injuries?\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to print the returned documents along with their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will enter a search phrase of your choice and perform a similarity search on that phrase.\n",
    "\n",
    "Set the *query* variable to a word or phrase of your choice; you can reference keywords from some of the case reports you saw in the data set when initially browsing it, if you'd like. Or you can choose any phrase you wish. Then, run the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Pulmonary diseases\"\n",
    "docs_with_score = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the block below to print the most similar results. Observe the similarity scores, keeping in mind that the closer to 0 it is, the closer in similarity the document is to your search phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with chunk sizes\n",
    "Next, let's try experimenting with varied chunk sizes. Run the block below, which is the same as your previous iteration for loading and chunking, but this time uses a chunk size of 400 instead of 2,500. Observe the change in the number of chunks that were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data from json lines file\n",
    "loader = JSONLoader(\n",
    "    file_path='./data/healthcare/augmented_notes_100.jsonl',\n",
    "    jq_schema='.note',\n",
    "    json_lines=True\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Got {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load this newly chunked data into a separate data store within InterSystems IRIS.\n",
    "\n",
    "Once again, this may take several moments while embeddings are created for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings()\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "\n",
    "COLLECTION_NAME = \"case_reports-chunked\"\n",
    "dbchunked = IRISVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")\n",
    "\n",
    "print(\"Embeddings generation for case reports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm how many documents are loaded into this second data structure in InterSystems IRIS by running the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of docs in vector store: {len(dbchunked.get()['ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try another vector search against the more finely-chunked data. Run the module below, optionally replacing your search term with the one you chose earlier. Whichever term you choose, observe how the results and similarity scores may differ slightly from before, when larger chunks were used.\n",
    "\n",
    "Also, you may notice that separate chunks can be part of the same case report. In practice, you may leverage a separate lookup table that could synthesize chunks alongside their original records. Here, we are simply returning the most relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Have any children presented with knee injuries?\"\n",
    "docs_with_score = dbchunked.similarity_search_with_score(query)\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare retrieval mechanism for RAG app\n",
    "Now that we have experimented with vector searches and chunk sizes, let's set up a retriever for our database. A retriever is an essential component in information retrieval systems, as it allows us to fetch relevant documents based on a query efficiently. By converting the database into a retriever, we enhance our ability to interact with the data, enabling more advanced search and retrieval operations.\n",
    "\n",
    "The `as_retriever()` method transforms the database into a retriever object. This object can then be used to perform various retrieval tasks, making it a versatile tool for working with our embedded documents.\n",
    "\n",
    "Run the block below to create the retriever and print it to confirm its setup. We'll use the more finely chunked data, consisting of 764 chunks, for this retriever.\n",
    "\n",
    "This retriever will be used for the RAG application later in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = dbchunked.as_retriever()\n",
    "print(retriever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
