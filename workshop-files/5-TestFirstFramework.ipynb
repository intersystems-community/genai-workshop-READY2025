{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test-First Framework for RAG Evaluation\n",
    "\n",
    "In this section, we'll implement a comprehensive evaluation framework using **DeepEval**, a modern evaluation library specifically designed for LLM applications and RAG systems.\n",
    "\n",
    "### Why DeepEval for RAG Evaluation?\n",
    "\n",
    "DeepEval provides several advantages:\n",
    "1. **RAG-Specific Metrics**: Built-in metrics for answer relevancy, faithfulness, and contextual recall\n",
    "2. **Synthetic Data Generation**: Automatically generate test cases from your knowledge base\n",
    "3. **LLM-as-a-Judge**: Uses advanced LLMs to evaluate responses intelligently\n",
    "4. **Easy Integration**: Simple API that works well with existing RAG pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this evaluation is not tightly integrated with the app you built in Sections 1-4, it evaluates a retrieval mechanism from the same data set and could be repurposed later in your own use cases for experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the module below to import the required libraries, create the InterSystems IRIS connection, and initialize the data retrieval mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# DeepEval imports\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    ContextualRecallMetric\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# Langchain imports for our RAG system\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Database connection details\n",
    "username = '_SYSTEM'\n",
    "password = 'SYS'\n",
    "hostname = 'IRIS'\n",
    "port = 1972\n",
    "namespace = 'IRISAPP'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "COLLECTION_NAME = \"case_reports\"\n",
    "\n",
    "# Initialize components\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "db = IRISVector(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING\n",
    ")\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "print(f\"Retriever initialized: {retriever}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the module below to load an OpenAI API key that has been prepared for this testing environment, as well as choose the OpenAI model (GPT 3.5 Turbo) that this section will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and check API key\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âš ï¸ Warning: OPENAI_API_KEY not found. Please set your OpenAI API key.\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key found. DeepEval is ready to use.\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Simple RAG Pipeline\n",
    "Now let's create a simple RAG pipeline to generate Q&A pairs for our data set. First, we'll create a pre-baked question and utilize RAG to retrieve the answer from our case reports data.\n",
    "\n",
    "Inspect the block of code below, then run it to create the first Q&A pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_pipeline(question: str, retriever, llm) -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Simple RAG pipeline that retrieves relevant documents and generates an answer.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    contexts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    # Create prompt and generate answer\n",
    "    context_text = \"\\n\\n\".join(contexts)\n",
    "    prompt = f\"\"\"\n",
    "Based on the following medical case reports, answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content, contexts\n",
    "\n",
    "# Test the pipeline\n",
    "test_question = \"What are common symptoms of knee problems in young patients?\"\n",
    "test_answer, test_contexts = simple_rag_pipeline(test_question, retriever, llm)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {test_answer}\")\n",
    "print(f\"Retrieved {len(test_contexts)} contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Chunked Data for Synthetic Test Generation\n",
    "Next, let's create three manual Q&A pairs and load them into a data structure. Read through the Q&A pairs below; for your own use cases, you might leverage domain experts to curate relevant Q&A pairs that can serve as \"gold-standard\" examples of the results your retrieval mechanism should yield.\n",
    "\n",
    "Run the block below to load these three manual pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manual test cases for evaluation\n",
    "manual_test_cases = [\n",
    "    {\n",
    "        \"input\": \"What are common symptoms of knee problems in young patients?\",\n",
    "        \"expected_output\": \"Common symptoms include pain, swelling, limited range of motion, and difficulty with weight-bearing activities.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How are fractures typically treated in elderly patients?\",\n",
    "        \"expected_output\": \"Treatment often involves surgical fixation, pain management, and careful consideration of the patient's overall health status.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What diagnostic methods are used for abdominal pain?\",\n",
    "        \"expected_output\": \"Common diagnostic methods include physical examination, CT scans, ultrasound, and laboratory tests.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(manual_test_cases)} test cases for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's augment our three manual Q&A pairs with some generated Q&A pairs. To do this, we'll retrieve 20 chunks from our data set to use as the basis for creating three more test questions and answers.\n",
    "\n",
    "Run the block below to load chunks from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunked documents from IRIS database for synthetic data generation\n",
    "# Using the chunked data that's already in our database (SQLUser.case_reports_chunked)\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Connect to IRIS and get chunked documents\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "# Query the chunked data table\n",
    "query = \"\"\"\n",
    "SELECT TOP 20 document, metadata \n",
    "FROM SQLUser.\\\"case_reports-chunked\\\" \n",
    "WHERE LENGTH(document) > 100\n",
    "ORDER BY id\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query))\n",
    "        chunked_data = result.fetchall()\n",
    "\n",
    "    # Convert to documents format for DeepEval\n",
    "    chunked_documents = []\n",
    "    for document, metadata in chunked_data:\n",
    "        doc = Document(\n",
    "            page_content=document,\n",
    "            metadata={\"source_metadata\": metadata}\n",
    "        )\n",
    "        chunked_documents.append(doc)\n",
    "\n",
    "    print(f\"âœ… Loaded {len(chunked_documents)} chunked documents from IRIS database\")\n",
    "    if chunked_documents:\n",
    "        print(f\"Sample chunk preview: {chunked_documents[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {str(chunked_documents[0].metadata['source_metadata'])[:100]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading chunked data: {e}\")\n",
    "    print(\"Will use manual test cases instead.\")\n",
    "    chunked_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate Synthetic Test Cases\n",
    "Using the 20 chunks we loaded, along with DeepEval's Synthesizer, let's generate three more realistic question-answer pairs.\n",
    "\n",
    "Run the block below to begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to generate synthetic test cases using DeepEval's Synthesizer\n",
    "synthetic_test_cases = []\n",
    "\n",
    "if chunked_documents and os.getenv(\"OPENAI_API_KEY\"):\n",
    "    try:\n",
    "        print(\"Generating synthetic test cases from chunked documents...\")\n",
    "        \n",
    "        # Initialize the DeepEval synthesizer\n",
    "        synthesizer = Synthesizer()\n",
    "        \n",
    "        # Use first 3 chunks for synthesis (to manage API costs)\n",
    "        contexts_for_synthesis = [[doc.page_content] for doc in chunked_documents[:3]]\n",
    "        \n",
    "        # Generate synthetic test cases using correct format: List[List[str]]\n",
    "        synthetic_test_cases = synthesizer.generate_goldens_from_contexts(\n",
    "            contexts=contexts_for_synthesis,  # List of lists of strings\n",
    "            max_goldens_per_context=1  # Generate 1 test case per context\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Generated {len(synthetic_test_cases)} synthetic test cases\")\n",
    "        \n",
    "        # Display a sample\n",
    "        if synthetic_test_cases:\n",
    "            sample = synthetic_test_cases[0]\n",
    "            print(f\"\\nSample Synthetic Test Case:\")\n",
    "            print(f\"Input: {sample.input}\")\n",
    "            print(f\"Expected Output: {sample.expected_output}\")\n",
    "            print(f\"Context: {sample.context[:150]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating synthetic test cases: {e}\")\n",
    "        print(\"This might be due to API rate limits. Using manual test cases instead.\")\n",
    "        synthetic_test_cases = []\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping synthetic generation (no chunked data or API key). Using manual test cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the block below to combine our manual Q&A pairs and the synthetically generated ones into a single data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine synthetic and manual test cases\n",
    "all_test_cases = []\n",
    "\n",
    "# Add synthetic test cases if available\n",
    "if synthetic_test_cases:\n",
    "    for case in synthetic_test_cases:\n",
    "        all_test_cases.append({\n",
    "            \"input\": case.input,\n",
    "            \"expected_output\": case.expected_output,\n",
    "            \"source\": \"synthetic\"\n",
    "        })\n",
    "\n",
    "# Add manual test cases as fallback\n",
    "manual_test_cases = [\n",
    "    {\n",
    "        \"input\": \"What are common symptoms of knee problems in young patients?\",\n",
    "        \"expected_output\": \"Common symptoms include pain, swelling, limited range of motion, and difficulty with weight-bearing activities.\",\n",
    "        \"source\": \"manual\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How are fractures typically treated in elderly patients?\",\n",
    "        \"expected_output\": \"Treatment often involves surgical fixation, pain management, and careful consideration of the patient's overall health status.\",\n",
    "        \"source\": \"manual\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What diagnostic methods are used for abdominal pain?\",\n",
    "        \"expected_output\": \"Common diagnostic methods include physical examination, CT scans, ultrasound, and laboratory tests.\",\n",
    "        \"source\": \"manual\"\n",
    "    }\n",
    "]\n",
    "\n",
    "all_test_cases.extend(manual_test_cases)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Total test cases: {len(all_test_cases)}\")\n",
    "synthetic_count = len([tc for tc in all_test_cases if tc['source'] == 'synthetic'])\n",
    "manual_count = len([tc for tc in all_test_cases if tc['source'] == 'manual'])\n",
    "print(f\"  - Synthetic: {synthetic_count}\")\n",
    "print(f\"  - Manual: {manual_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run RAG Pipeline on Test Cases\n",
    "With six Q&A pairs createdâ€”three manually curated, three synthetically generatedâ€”let's run the simple RAG pipeline to simulate retrieval of relevant documents and generation of answers to the test questions. Run the block below; this may take a few moments as each of the six test cases is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG pipeline on test cases\n",
    "evaluation_results = []\n",
    "\n",
    "for i, test_case in enumerate(all_test_cases):\n",
    "    print(f\"Processing test case {i+1}/{len(all_test_cases)}...\")\n",
    "    \n",
    "    question = test_case[\"input\"]\n",
    "    expected_answer = test_case[\"expected_output\"]\n",
    "    \n",
    "    try:\n",
    "        actual_answer, retrieved_contexts = simple_rag_pipeline(question, retriever, llm)\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            \"question\": question,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"retrieved_contexts\": retrieved_contexts\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing test case {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully processed {len(evaluation_results)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate with DeepEval Metrics\n",
    "We've now processed the six test questions with a simple RAG retrieval and generation to answer the test questions. Now, we can leverage DeepEval to test the retrieval and answers. In the block below, we'll initialize the DeepEval metrics we will measure.\n",
    "\n",
    "These DeepEval metrics are designed to evaluate the quality of responses generated by a language model, particularly in retrieval-augmented generation (RAG) systems. \n",
    "- **Answer Relevancy** measures how well the generated answer addresses the original question.\n",
    "- **Faithfulness** assesses whether the answer accurately reflects the retrieved context, ensuring it doesnâ€™t hallucinate or introduce unsupported information.\n",
    "- **Contextual Relevancy** checks how relevant the retrieved context is to the question.\n",
    "- **Contextual Recall** evaluates whether all key pieces of information needed to answer the question are present in the retrieved context.\n",
    "\n",
    "Together, these metrics help ensure that the model's answers are accurate, grounded, and contextually appropriate. Run the block below to initialize these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepEval metrics\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "faithfulness_metric = FaithfulnessMetric(threshold=0.7)\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(threshold=0.7)\n",
    "contextual_recall_metric = ContextualRecallMetric(threshold=0.7)\n",
    "\n",
    "# Create LLMTestCase objects for DeepEval\n",
    "test_cases_for_evaluation = []\n",
    "\n",
    "for result in evaluation_results:\n",
    "    test_case = LLMTestCase(\n",
    "        input=result[\"question\"],\n",
    "        actual_output=result[\"actual_answer\"],\n",
    "        expected_output=result[\"expected_answer\"],\n",
    "        retrieval_context=result[\"retrieved_contexts\"]\n",
    "    )\n",
    "    test_cases_for_evaluation.append(test_case)\n",
    "\n",
    "print(f\"Created {len(test_cases_for_evaluation)} test cases for DeepEval evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run the evaluation. This process may take several minutes; after running the block of code, feel free to grab a cup of coffee while these six test cases are evaluated for our desired metrics using DeepEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with DeepEval\n",
    "print(\"Running DeepEval evaluation...\")\n",
    "\n",
    "try:\n",
    "    evaluation_scores = {\n",
    "        \"answer_relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contextual_relevancy\": [],\n",
    "        \"contextual_recall\": []\n",
    "    }\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases_for_evaluation):\n",
    "        print(f\"Evaluating test case {i+1}/{len(test_cases_for_evaluation)}...\")\n",
    "        \n",
    "        # Evaluate each metric\n",
    "        answer_relevancy_metric.measure(test_case)\n",
    "        evaluation_scores[\"answer_relevancy\"].append(answer_relevancy_metric.score)\n",
    "        \n",
    "        faithfulness_metric.measure(test_case)\n",
    "        evaluation_scores[\"faithfulness\"].append(faithfulness_metric.score)\n",
    "        \n",
    "        contextual_relevancy_metric.measure(test_case)\n",
    "        evaluation_scores[\"contextual_relevancy\"].append(contextual_relevancy_metric.score)\n",
    "        \n",
    "        contextual_recall_metric.measure(test_case)\n",
    "        evaluation_scores[\"contextual_recall\"].append(contextual_recall_metric.score)\n",
    "    \n",
    "    print(\"âœ… Evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during evaluation: {e}\")\n",
    "    # Create dummy scores for demonstration\n",
    "    evaluation_scores = {\n",
    "        \"answer_relevancy\": [0.8, 0.7, 0.9],\n",
    "        \"faithfulness\": [0.85, 0.75, 0.8],\n",
    "        \"contextual_relevancy\": [0.7, 0.8, 0.85],\n",
    "        \"contextual_recall\": [0.75, 0.7, 0.8]\n",
    "    }\n",
    "    print(\"Using dummy scores for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze and Visualize Results\n",
    "In addition to calculating the evaluation scores, it can be very helpful to visualize the results to quickly spot strengths and weaknesses in your RAG system's performance. Graphs like bar charts and radar plots make it easier to compare metrics side by side, highlight areas that may need improvement (such as low faithfulness or contextual recall), and communicate findings more effectively to others. Before diving into numerical details, visualizations offer an intuitive overview that supports more informed analysis and debugging.\n",
    "\n",
    "Run the block below to visualize your evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate average scores\n",
    "avg_scores = {}\n",
    "for metric, scores in evaluation_scores.items():\n",
    "    avg_scores[metric] = np.mean(scores) if scores else 0\n",
    "\n",
    "print(\"ðŸ“Š RAG System Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, avg_score in avg_scores.items():\n",
    "    print(f\"{metric.replace('_', ' ').title()}: {avg_score:.3f}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('RAG System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "metrics = list(avg_scores.keys())\n",
    "scores = list(avg_scores.values())\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "bars = ax1.bar(metrics, scores, color=colors)\n",
    "ax1.set_title('Average Evaluation Scores')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)\n",
    "scores_radar = list(avg_scores.values())\n",
    "scores_radar += scores_radar[:1]\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "ax2.plot(angles, scores_radar, 'o-', linewidth=2, color='#FF6B6B')\n",
    "ax2.fill(angles, scores_radar, alpha=0.25, color='#FF6B6B')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('RAG Performance Radar')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Recommendations\n",
    "To wrap up the evaluation, itâ€™s useful to generate a concise performance summary that highlights your systemâ€™s strongest and weakest areas. This overview helps prioritize improvements by identifying which metrics are performing well and which may need more attention. By pairing each score with actionable recommendations, you can start to make targeted adjustmentsâ€”whether that means refining your retrieval process, improving prompt construction, or tweaking how documents are chunked. The summary below provides both a quick snapshot and practical next steps for improving your RAG systemâ€™s overall effectiveness.\n",
    "\n",
    "What do you notice about these results? Consider why a given metric might be low in this scenario. We'll touch on that in the conclusion, after running the code block below to generate a summary and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"\\nðŸ” Performance Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "best_metric = max(avg_scores, key=avg_scores.get)\n",
    "worst_metric = min(avg_scores, key=avg_scores.get)\n",
    "overall_avg = np.mean(list(avg_scores.values()))\n",
    "\n",
    "print(f\"ðŸŽ¯ Best Performing Metric: {best_metric.replace('_', ' ').title()}\")\n",
    "print(f\"   Score: {avg_scores[best_metric]:.3f}\")\n",
    "print(f\"\\nðŸ”§ Needs Improvement: {worst_metric.replace('_', ' ').title()}\")\n",
    "print(f\"   Score: {avg_scores[worst_metric]:.3f}\")\n",
    "print(f\"\\nðŸ“Š Overall Average: {overall_avg:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Improvement Recommendations:\")\n",
    "print(\"â€¢ Scores > 0.8: Excellent performance\")\n",
    "print(\"â€¢ Scores 0.7-0.8: Good performance\")\n",
    "print(\"â€¢ Scores < 0.7: Needs improvement\")\n",
    "\n",
    "if avg_scores['answer_relevancy'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Answer Relevancy Tips:\")\n",
    "    print(\"  - Improve prompt engineering\")\n",
    "    print(\"  - Add question classification\")\n",
    "\n",
    "if avg_scores['faithfulness'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Faithfulness Tips:\")\n",
    "    print(\"  - Improve retrieval quality\")\n",
    "    print(\"  - Add explicit context adherence instructions\")\n",
    "\n",
    "if avg_scores['contextual_relevancy'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Contextual Relevancy Tips:\")\n",
    "    print(\"  - Optimize embedding model\")\n",
    "    print(\"  - Tune retrieval parameters\")\n",
    "\n",
    "if avg_scores['contextual_recall'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Contextual Recall Tips:\")\n",
    "    print(\"  - Increase number of retrieved documents\")\n",
    "    print(\"  - Improve document chunking strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "You may have noticed that our **Contextual Relevancy** score was quite unimpressive in this scenario. This is likely due to using such a limited data set in this sample exercise -- we have only 100 case reports stored, which is unlikely to be enough to answer generalized questions about patient conditions. Using evaluation tools like this can help to highlight pieces of your application that need attention.\n",
    "\n",
    "This test-first framework using DeepEval provides:\n",
    "\n",
    "1. **Objective Measurement**: Quantitative metrics for RAG system performance\n",
    "2. **Systematic Improvement**: Data-driven insights for optimization\n",
    "3. **Regression Detection**: Ability to catch performance degradation\n",
    "4. **Comparative Analysis**: Framework for comparing different approaches\n",
    "\n",
    "Use this evaluation framework throughout your RAG development process to ensure consistent quality and continuous improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
