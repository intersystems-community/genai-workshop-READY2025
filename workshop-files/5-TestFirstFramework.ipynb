{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a Test-First Framework for RAG Evaluation\n",
    "\n",
    "In this section, we'll implement a comprehensive evaluation framework using DeepEval, a modern evaluation library specifically designed for LLM applications and RAG systems.\n",
    "\n",
    "### Why DeepEval for RAG Evaluation?\n",
    "\n",
    "DeepEval provides several advantages:\n",
    "\n",
    "1. **RAG-Specific Metrics:** Built-in metrics for answer relevancy, faithfulness, and contextual recall\n",
    "2. **Synthetic Data Generation:** Automatically generate test cases from your knowledge base\n",
    "3. **LLM-as-a-Judge:** Uses advanced LLMs to evaluate responses intelligently\n",
    "4. **Easy Integration:** Simple API that works well with existing RAG pipelines\n",
    "\n",
    "While this evaluation is not tightly integrated with the app you built in Sections 1-4, it evaluates a retrieval mechanism from the same data set and could be repurposed later in your own use cases for experimentation.\n",
    "\n",
    "Run the module below to import the required libraries, create the InterSystems IRIS connection, and initialize the data retrieval mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# DeepEval imports\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    ContextualRecallMetric\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# Langchain imports for our RAG system\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Database connection details\n",
    "username = '_SYSTEM'\n",
    "password = 'SYS'\n",
    "hostname = 'IRIS'\n",
    "port = 1972\n",
    "namespace = 'IRISAPP'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "COLLECTION_NAME = \"case_reports\"\n",
    "\n",
    "# Initialize components\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "db = IRISVector(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING\n",
    ")\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "print(f\"Retriever initialized: {retriever}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the module below to load an OpenAI API key that has been prepared for this section, as well as set the OpenAI model that this section will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and check API key\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# For this segment we will set a new API Key\n",
    "from utils import LLM_MODEL\n",
    "os.environ['OPENAI_API_KEY'] = os.environ['OPENAI_API_KEY2']\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY not found. Please set your OpenAI API key.\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key found. DeepEval is ready to use.\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=LLM_MODEL, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Replicate Your App's RAG Pipeline\n",
    "Now let's replicate the exact RAG pipeline from the chat application you have build during this workshop. We'll use this RAG pipeline to generate Q&A pairs for our data set. First, we'll create a pre-baked question and utilize RAG to retrieve the answer from our case reports data.\n",
    "\n",
    "Inspect the block of code below, then run it to create the first Q&A pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our shared RAG module\n",
    "from rag_module import WorkshopRAG\n",
    "\n",
    "# Initialize the RAG system (same as used in chat app)\n",
    "print(\"üîß Initializing shared RAG system...\")\n",
    "rag_system = WorkshopRAG(\n",
    "    collection_name=\"case_reports\",\n",
    "    llm_model=LLM_MODEL,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "def workshop_chat_rag_pipeline(question: str) -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    RAG pipeline that uses the exact same module as the chat application.\n",
    "    This ensures we're evaluating the identical system users interact with.\n",
    "    \"\"\"\n",
    "    return rag_system.query(question)\n",
    "\n",
    "# Test the pipeline\n",
    "test_question = \"What are common symptoms of knee problems in adult patients?\"\n",
    "test_answer, test_contexts = workshop_chat_rag_pipeline(test_question)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {test_answer}\")\n",
    "print(f\"Retrieved {len(test_contexts)} contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Chunked Data for Synthetic Test Generation\n",
    "Next, let's create three manual Q&A pairs and load them into a data structure. Read through the Q&A pairs below; for your own use cases, you might leverage domain experts to curate relevant Q&A pairs that can serve as \"gold-standard\" examples of the results your retrieval mechanism should yield.\n",
    "\n",
    "Run the block below to load these three manual pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manual test cases for evaluation\n",
    "manual_test_cases = [\n",
    "    {\n",
    "        \"input\": \"What are common symptoms of knee problems in adult patients?\",\n",
    "        \"expected_output\": \"Common symptoms include pain, swelling, limited range of motion, and difficulty with weight-bearing activities.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How are fractures typically treated in elderly patients?\",\n",
    "        \"expected_output\": \"Treatment often involves surgical fixation, pain management, and careful consideration of the patient's overall health status.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What diagnostic methods are used for abdominal pain?\",\n",
    "        \"expected_output\": \"Common diagnostic methods include physical examination, CT scans, ultrasound, and laboratory tests.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(manual_test_cases)} test cases for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's augment our three manual Q&A pairs with some generated Q&A pairs. To do this, we'll retrieve 20 chunks from our data set to use as the basis for creating three more test questions and answers.\n",
    "\n",
    "Run the block below to load chunks from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunked documents from IRIS database for synthetic data generation\n",
    "# Using the chunked data that's already in our database (SQLUser.case_reports_chunked)\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Connect to IRIS and get chunked documents\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "# Query the chunked data table\n",
    "query = \"\"\"\n",
    "SELECT TOP 20 document, metadata \n",
    "FROM SQLUser.\\\"case_reports-chunked\\\" \n",
    "WHERE LENGTH(document) > 100\n",
    "ORDER BY id\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query))\n",
    "        chunked_data = result.fetchall()\n",
    "\n",
    "    # Convert to documents format for DeepEval\n",
    "    chunked_documents = []\n",
    "    for document, metadata in chunked_data:\n",
    "        doc = Document(\n",
    "            page_content=document,\n",
    "            metadata={\"source_metadata\": metadata}\n",
    "        )\n",
    "        chunked_documents.append(doc)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(chunked_documents)} chunked documents from IRIS database\")\n",
    "    if chunked_documents:\n",
    "        print(f\"Sample chunk preview: {chunked_documents[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {str(chunked_documents[0].metadata['source_metadata'])[:100]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading chunked data: {e}\")\n",
    "    print(\"Will use manual test cases instead.\")\n",
    "    chunked_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate Synthetic Test Cases\n",
    "Instead of manually writing test questions, we can use AI to automatically generate realistic test cases from our actual medical data. Here's how it works:\n",
    "\n",
    "1. **Input**: Real medical case chunks from our IRIS database\n",
    "2. **Process**: DeepEval's Synthesizer uses GPT-4 to create questions that could realistically be asked about this data\n",
    "3. **Output**: Question-answer pairs with expected responses\n",
    "\n",
    "**Why is this useful?**\n",
    "- Creates test cases that match your actual data\n",
    "- Saves time compared to manual test creation\n",
    "- Generates diverse question types you might not think of\n",
    "- Ensures evaluation covers real scenarios your users will encounter\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to generate synthetic test cases using DeepEval's Synthesizer\n",
    "synthetic_test_cases = []\n",
    "\n",
    "if chunked_documents and os.getenv(\"OPENAI_API_KEY\"):\n",
    "    try:\n",
    "        print(\"\\nüìä STEP 2: Preparing our medical data for AI analysis...\\n   ‚Üí We have {len(chunked_documents)} chunks of medical case data\")\n",
    "        \n",
    "        # Initialize the DeepEval synthesizer\n",
    "        print(f\"   ‚Üí AI synthesizer ready (powered by {LLM_MODEL})\")\n",
    "        synthesizer = Synthesizer(model=LLM_MODEL)\n",
    "        \n",
    "        # Select chunks for test question generation\n",
    "        print(\"\\nüß† STEP 3: AI is analyzing medical data and creating test questions...\")\n",
    "        print(\"   (This may take 10-30 seconds as GPT-4 reads and understands the medical content)\")\n",
    "        contexts_for_synthesis = [[doc.page_content] for doc in chunked_documents[:3]]\n",
    "        # Use first 3 chunks for synthesis (to manage API costs)\n",
    "        print(f\"   ‚Üí Selected {len(contexts_for_synthesis)} chunks for test generation (to manage costs)\")\n",
    "        \n",
    "        # Generate synthetic test cases using correct format: List[List[str]]\n",
    "        synthetic_test_cases = synthesizer.generate_goldens_from_contexts(\n",
    "            contexts=contexts_for_synthesis,  # List of lists of strings\n",
    "            max_goldens_per_context=1  # Generate 1 test case per context\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ SUCCESS: Generated {len(synthetic_test_cases)} synthetic test cases!\")\n",
    "        print(\"   ‚Üí Each test case contains: Question + Expected Answer + Source Context\")\n",
    "        \n",
    "        # üìã STEP 4: Show what the AI created\n",
    "        if synthetic_test_cases:\n",
    "            print(\"\\nüìã STEP 4: Let's examine what the AI generated...\")\n",
    "            sample = synthetic_test_cases[0]\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üîç EXAMPLE SYNTHETIC TEST CASE:\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"‚ùì QUESTION: {sample.input}\")\n",
    "            print(f\"\\nüí° EXPECTED ANSWER: {sample.expected_output}\")\n",
    "            print(f\"\\nüìÑ SOURCE CONTEXT: {sample.context[:150]}...\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"\\nüí≠ Notice how the AI:\")\n",
    "            print(\"   ‚Ä¢ Created a realistic medical question from the data\")\n",
    "            print(\"   ‚Ä¢ Generated an appropriate expected answer\")\n",
    "            print(\"   ‚Ä¢ Linked it to the specific source context\")\n",
    "            print(\"   ‚Ä¢ This mimics real user questions about medical cases!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating synthetic test cases: {e}\")\n",
    "        print(\"This might be due to API rate limits. Using manual test cases instead.\")\n",
    "        synthetic_test_cases = []\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping synthetic generation (no chunked data or API key). Using manual test cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the block below to combine our manual Q&A pairs and the synthetically generated ones into a single data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ STEP 1: Create our master test suite\n",
    "print(\"üîÑ Combining AI-generated and manual test cases...\")\n",
    "all_test_cases = []\n",
    "\n",
    "# Add synthetic test cases if available\n",
    "if synthetic_test_cases:\n",
    "    print(f\"   ‚Üí Adding {len(synthetic_test_cases)} AI-generated test cases\")\n",
    "    for case in synthetic_test_cases:\n",
    "        all_test_cases.append({\n",
    "            \"input\": case.input,\n",
    "            \"expected_output\": case.expected_output,\n",
    "            \"source\": \"synthetic\"  # Mark as AI-generated\n",
    "        })\n",
    "else:\n",
    "    print(\"   ‚Üí No synthetic test cases available (using manual only)\")\n",
    "\n",
    "# üìù STEP 2: Add manually crafted test cases\n",
    "print(\"\\nüìù Adding manually crafted test cases...\")\n",
    "manual_test_cases = [\n",
    "    {\n",
    "        \"input\": \"What are common symptoms of knee problems in young patients?\",\n",
    "        \"expected_output\": \"Common symptoms include pain, swelling, limited range of motion, and difficulty with weight-bearing activities.\",\n",
    "        \"source\": \"manual\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How are fractures typically treated in elderly patients?\",\n",
    "        \"expected_output\": \"Treatment often involves surgical fixation, pain management, and careful consideration of the patient's overall health status.\",\n",
    "        \"source\": \"manual\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What diagnostic methods are used for abdominal pain?\",\n",
    "        \"expected_output\": \"Common diagnostic methods include physical examination, CT scans, ultrasound, and laboratory tests.\",\n",
    "        \"source\": \"manual\"\n",
    "    }\n",
    "]\n",
    "\n",
    "all_test_cases.extend(manual_test_cases)\n",
    "\n",
    "print(f\"\\nüìã Total test cases: {len(all_test_cases)}\")\n",
    "synthetic_count = len([tc for tc in all_test_cases if tc['source'] == 'synthetic'])\n",
    "manual_count = len([tc for tc in all_test_cases if tc['source'] == 'manual'])\n",
    "print(f\"  - Synthetic: {synthetic_count}\")\n",
    "print(f\"  - Manual: {manual_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run RAG Pipeline on Test Cases\n",
    "\n",
    "Now we'll test our chat application's RAG pipeline with each test case. This step:\n",
    "\n",
    "1. **Feeds each question** to our workshop chat RAG pipeline\n",
    "2. **Collects the actual answers** generated by our system\n",
    "3. **Captures the retrieved contexts** used for each answer\n",
    "4. **Prepares data** for DeepEval's evaluation metrics\n",
    "\n",
    "**What we're testing:** The exact same RAG system users interact with in the chat application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ STEP 1: Run our chat RAG pipeline on each test case\n",
    "print(\"üß™ Testing our chat application with all test cases...\")\n",
    "print(\"   (This will take a few moments as we query the LLM for each test case)\")\n",
    "evaluation_results = []\n",
    "\n",
    "for i, test_case in enumerate(all_test_cases):\n",
    "    print(f\"\\nüìù Processing test case {i+1}/{len(all_test_cases)}...\")\n",
    "    print(f\"   Question: {test_case['input'][:80]}...\")\n",
    "    \n",
    "    question = test_case[\"input\"]\n",
    "    expected_answer = test_case[\"expected_output\"]\n",
    "    \n",
    "    try:\n",
    "        actual_answer, retrieved_contexts = workshop_chat_rag_pipeline(question)\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            \"question\": question,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"retrieved_contexts\": retrieved_contexts\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing test case {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ SUCCESS: Processed {len(evaluation_results)} test cases!\")\n",
    "print(\"   ‚Üí Each test case now has: Question + Expected Answer + Actual Answer + Retrieved Context\")\n",
    "print(\"   ‚Üí Ready for DeepEval metrics evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate with DeepEval Metrics\n",
    "We've now processed the six test questions with a simple RAG retrieval and generation to answer the test questions. Now, we can leverage DeepEval to test the retrieval and answers. In the block below, we'll initialize the DeepEval metrics we will measure.\n",
    "\n",
    "These DeepEval metrics are designed to evaluate the quality of responses generated by a language model, particularly in retrieval-augmented generation (RAG) systems.\n",
    "\n",
    "- **Answer Relevancy** measures how well the generated answer addresses the original question.\n",
    "- **Faithfulness** assesses whether the answer accurately reflects the retrieved context, ensuring it doesn‚Äôt hallucinate or introduce unsupported information.\n",
    "- **Contextual Relevancy** checks how relevant the retrieved context is to the question.\n",
    "- **Contextual Recall** evaluates whether all key pieces of information needed to answer the question are present in the retrieved context.\n",
    "\n",
    "Together, these metrics help ensure that the model's answers are accurate, grounded, and contextually appropriate. Run the block below to initialize these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepEval metrics\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model=LLM_MODEL)\n",
    "faithfulness_metric = FaithfulnessMetric(threshold=0.7, model=LLM_MODEL)\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(threshold=0.7, model=LLM_MODEL)\n",
    "contextual_recall_metric = ContextualRecallMetric(threshold=0.7, model=LLM_MODEL)\n",
    "\n",
    "# Create LLMTestCase objects for DeepEval\n",
    "test_cases_for_evaluation = []\n",
    "\n",
    "for result in evaluation_results:\n",
    "    test_case = LLMTestCase(\n",
    "        input=result[\"question\"],\n",
    "        actual_output=result[\"actual_answer\"],\n",
    "        expected_output=result[\"expected_answer\"],\n",
    "        retrieval_context=result[\"retrieved_contexts\"]\n",
    "    )\n",
    "    test_cases_for_evaluation.append(test_case)\n",
    "\n",
    "print(f\"Created {len(test_cases_for_evaluation)} test cases for DeepEval evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run the evaluation. This process may take several minutes; after running the block of code, feel free to grab a cup of coffee while these six test cases are evaluated for our desired metrics using DeepEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with DeepEval\n",
    "print(\"Running DeepEval evaluation...\")\n",
    "\n",
    "try:\n",
    "    evaluation_scores = {\n",
    "        \"answer_relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contextual_relevancy\": [],\n",
    "        \"contextual_recall\": []\n",
    "    }\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases_for_evaluation):\n",
    "        print(f\"Evaluating test case {i+1}/{len(test_cases_for_evaluation)}...\")\n",
    "        \n",
    "        # Evaluate each metric\n",
    "        answer_relevancy_metric.measure(test_case)\n",
    "        evaluation_scores[\"answer_relevancy\"].append(answer_relevancy_metric.score)\n",
    "        \n",
    "        faithfulness_metric.measure(test_case)\n",
    "        evaluation_scores[\"faithfulness\"].append(faithfulness_metric.score)\n",
    "        \n",
    "        contextual_relevancy_metric.measure(test_case)\n",
    "        evaluation_scores[\"contextual_relevancy\"].append(contextual_relevancy_metric.score)\n",
    "        \n",
    "        contextual_recall_metric.measure(test_case)\n",
    "        evaluation_scores[\"contextual_recall\"].append(contextual_recall_metric.score)\n",
    "    \n",
    "    print(\"‚úÖ Evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during evaluation: {e}\")\n",
    "    # Create dummy scores for demonstration\n",
    "    evaluation_scores = {\n",
    "        \"answer_relevancy\": [0.8, 0.7, 0.9],\n",
    "        \"faithfulness\": [0.85, 0.75, 0.8],\n",
    "        \"contextual_relevancy\": [0.7, 0.8, 0.85],\n",
    "        \"contextual_recall\": [0.75, 0.7, 0.8]\n",
    "    }\n",
    "    print(\"Using dummy scores for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze and Visualize Results\n",
    "In addition to calculating the evaluation scores, it can be very helpful to visualize the results to quickly spot strengths and weaknesses in your RAG system's performance. Graphs like bar charts and radar plots make it easier to compare metrics side by side, highlight areas that may need improvement (such as low faithfulness or contextual recall), and communicate findings more effectively to others. Before diving into numerical details, visualizations offer an intuitive overview that supports more informed analysis and debugging.\n",
    "\n",
    "Run the block below to visualize your evaluation results. This step also may take a few moments to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate average scores\n",
    "avg_scores = {}\n",
    "for metric, scores in evaluation_scores.items():\n",
    "    avg_scores[metric] = np.mean(scores) if scores else 0\n",
    "\n",
    "print(\"üìä RAG System Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, avg_score in avg_scores.items():\n",
    "    print(f\"{metric.replace('_', ' ').title()}: {avg_score:.3f}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('RAG System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "metrics = list(avg_scores.keys())\n",
    "scores = list(avg_scores.values())\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "bars = ax1.bar(metrics, scores, color=colors)\n",
    "ax1.set_title('Average Evaluation Scores')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)\n",
    "scores_radar = list(avg_scores.values())\n",
    "scores_radar += scores_radar[:1]\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "ax2.plot(angles, scores_radar, 'o-', linewidth=2, color='#FF6B6B')\n",
    "ax2.fill(angles, scores_radar, alpha=0.25, color='#FF6B6B')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('RAG Performance Radar')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Recommendations\n",
    "To wrap up the evaluation, it‚Äôs useful to generate a concise performance summary that highlights your system‚Äôs strongest and weakest areas. This overview helps prioritize improvements by identifying which metrics are performing well and which may need more attention. By pairing each score with actionable recommendations, you can start to make targeted adjustments‚Äîwhether that means refining your retrieval process, improving prompt construction, or tweaking how documents are chunked. The summary below provides both a quick snapshot and practical next steps for improving your RAG system‚Äôs overall effectiveness.\n",
    "\n",
    "What do you notice about these results? Consider why a given metric might be low in this scenario. We'll touch on that in the conclusion, after running the code block below to generate a summary and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"\\nüîç Performance Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "best_metric = max(avg_scores, key=avg_scores.get)\n",
    "worst_metric = min(avg_scores, key=avg_scores.get)\n",
    "overall_avg = np.mean(list(avg_scores.values()))\n",
    "\n",
    "print(f\"üéØ Best Performing Metric: {best_metric.replace('_', ' ').title()}\")\n",
    "print(f\"   Score: {avg_scores[best_metric]:.3f}\")\n",
    "print(f\"\\nüîß Needs Improvement: {worst_metric.replace('_', ' ').title()}\")\n",
    "print(f\"   Score: {avg_scores[worst_metric]:.3f}\")\n",
    "print(f\"\\nüìä Overall Average: {overall_avg:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Improvement Recommendations:\")\n",
    "print(\"‚Ä¢ Scores > 0.8: Excellent performance\")\n",
    "print(\"‚Ä¢ Scores 0.7-0.8: Good performance\")\n",
    "print(\"‚Ä¢ Scores < 0.7: Needs improvement\")\n",
    "\n",
    "if avg_scores['answer_relevancy'] < 0.7:\n",
    "    print(\"\\nüîß Answer Relevancy Tips:\")\n",
    "    print(\"  - Improve prompt engineering\")\n",
    "    print(\"  - Add question classification\")\n",
    "\n",
    "if avg_scores['faithfulness'] < 0.7:\n",
    "    print(\"\\nüîß Faithfulness Tips:\")\n",
    "    print(\"  - Improve retrieval quality\")\n",
    "    print(\"  - Add explicit context adherence instructions\")\n",
    "\n",
    "if avg_scores['contextual_relevancy'] < 0.7:\n",
    "    print(\"\\nüîß Contextual Relevancy Tips:\")\n",
    "    print(\"  - Optimize embedding model\")\n",
    "    print(\"  - Tune retrieval parameters\")\n",
    "\n",
    "if avg_scores['contextual_recall'] < 0.7:\n",
    "    print(\"\\nüîß Contextual Recall Tips:\")\n",
    "    print(\"  - Increase number of retrieved documents\")\n",
    "    print(\"  - Improve document chunking strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "You may have noticed that our **Contextual Relevancy** score was quite unimpressive in this scenario, and the overall results need improvement. This is a reflection of the lightweight demo environment we are using -- with such a limited data set in this sample exercise (we have only 100 case reports stored), and a lightweight OpenAI model being used. This is unlikely to be enough to answer generalized questions about patient conditions. Using evaluation tools like this can help to highlight pieces of your application that need attention.\n",
    "\n",
    "This test-first framework using DeepEval provides:\n",
    "\n",
    "1. **Objective Measurement**: Quantitative metrics for RAG system performance\n",
    "2. **Systematic Improvement**: Data-driven insights for optimization\n",
    "3. **Regression Detection**: Ability to catch performance degradation\n",
    "4. **Comparative Analysis**: Framework for comparing different approaches\n",
    "\n",
    "Use this evaluation framework throughout your RAG development process to ensure consistent quality and continuous improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
