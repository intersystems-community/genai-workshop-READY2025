{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test-First Framework for RAG Evaluation\n",
    "\n",
    "In this notebook, we'll implement a comprehensive evaluation framework using **DeepEval**, a modern evaluation library specifically designed for LLM applications and RAG systems.\n",
    "\n",
    "### Why DeepEval for RAG Evaluation?\n",
    "\n",
    "DeepEval provides several advantages:\n",
    "1. **RAG-Specific Metrics**: Built-in metrics for answer relevancy, faithfulness, and contextual recall\n",
    "2. **Synthetic Data Generation**: Automatically generate test cases from your knowledge base\n",
    "3. **LLM-as-a-Judge**: Uses advanced LLMs to evaluate responses intelligently\n",
    "4. **Easy Integration**: Simple API that works well with existing RAG pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# DeepEval imports\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    ContextualRecallMetric\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# Langchain imports for our RAG system\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Database connection details\n",
    "username = '_SYSTEM'\n",
    "password = 'SYS'\n",
    "hostname = 'IRIS'\n",
    "port = 1972\n",
    "namespace = 'IRISAPP'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "COLLECTION_NAME = \"case_reports\"\n",
    "\n",
    "# Initialize components\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "db = IRISVector(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING\n",
    ")\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "print(f\"Retriever initialized: {retriever}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and check API key\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âš ï¸ Warning: OPENAI_API_KEY not found. Please set your OpenAI API key.\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key found. DeepEval is ready to use.\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Workshop Chat App RAG Pipeline\n",
     "\n",
     "This pipeline replicates the exact RAG system from the chat application built in previous notebooks. This way, you can evaluate the same system users interact with and see how changes to prompts affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workshop_chat_rag_pipeline(question: str, db, llm) -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Simple RAG pipeline that retrieves relevant documents and generates an answer.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    docs_with_score = db.similarity_search_with_score(question)\n",
    "    contexts = [doc.page_content for doc, _ in docs_with_score]\n",
    "    \n",
    "    # Create prompt and generate answer\n",
    "    context_text = \"\\n\\n\".join(contexts)\n",
    "    prompt = f\"\"\"\n",
    "Based on the following medical case reports, answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content, contexts\n",
    "\n",
    "# Test the pipeline\n",
    "test_question = \"What are common symptoms of knee problems in young patients?\"\n",
    "test_answer, test_contexts = workshop_chat_rag_pipeline(test_question, db, llm)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {test_answer}\")\n",
    "print(f\"Retrieved {len(test_contexts)} contexts\")"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "#### ðŸ”„ **Integration with Workshop Chat Application**\n",
     "\n",
     "The `workshop_chat_rag_pipeline` function above replicates the same RAG system you built in the previous notebooks. This means:\n",
     "\n",
     "- **Same Vector Search**: Uses `similarity_search_with_score()` like your chat app\n",
     "- **Same Data Source**: Connects to the same IRIS database and collection\n",
     "- **Same Structure**: Follows the exact pattern from `Chat3-GuardrailsAndHistory.py`\n",
     "\n",
     "**ðŸ’¡ Experiment with Changes:**\n",
     "1. **Modify the prompt template** in the function above\n",
     "2. **Run the evaluation** below to see performance changes\n",
     "3. **Compare results** to understand what works better\n",
     "4. **Apply successful changes** back to your chat application\n",
     "\n",
     "This creates a feedback loop: *Build â†’ Evaluate â†’ Improve â†’ Repeat*"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Chunked Data for Synthetic Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manual test cases for evaluation\n",
    "manual_test_cases = [\n",
    "    {\n",
    "        \"input\": \"What are common symptoms of knee problems in young patients?\",\n",
    "        \"expected_output\": \"Common symptoms include pain, swelling, limited range of motion, and difficulty with weight-bearing activities.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How are fractures typically treated in elderly patients?\",\n",
    "        \"expected_output\": \"Treatment often involves surgical fixation, pain management, and careful consideration of the patient's overall health status.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What diagnostic methods are used for abdominal pain?\",\n",
    "        \"expected_output\": \"Common diagnostic methods include physical examination, CT scans, ultrasound, and laboratory tests.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(manual_test_cases)} test cases for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunked documents from IRIS database for synthetic data generation\n",
    "# Using the chunked data that's already in our database (SQLUser.case_reports_chunked)\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Connect to IRIS and get chunked documents\n",
    "engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "# Query the chunked data table\n",
    "query = \"\"\"\n",
    "SELECT TOP 20 document, metadata \n",
    "FROM SQLUser.\\\"case_reports-chunked\\\" \n",
    "WHERE LENGTH(document) > 100\n",
    "ORDER BY id\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query))\n",
    "        chunked_data = result.fetchall()\n",
    "\n",
    "    # Convert to documents format for DeepEval\n",
    "    chunked_documents = []\n",
    "    for document, metadata in chunked_data:\n",
    "        doc = Document(\n",
    "            page_content=document,\n",
    "            metadata={\"source_metadata\": metadata}\n",
    "        )\n",
    "        chunked_documents.append(doc)\n",
    "\n",
    "    print(f\"âœ… Loaded {len(chunked_documents)} chunked documents from IRIS database\")\n",
    "    if chunked_documents:\n",
    "        print(f\"Sample chunk preview: {chunked_documents[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {str(chunked_documents[0].metadata['source_metadata'])[:100]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading chunked data: {e}\")\n",
    "    print(\"Will use manual test cases instead.\")\n",
    "    chunked_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Generate Synthetic Test Cases (Optional)\n",
    "\n",
    "**What is Synthetic Test Generation?**\n",
     "\n",
     "Instead of manually writing test questions, we can use AI to automatically generate realistic test cases from our actual medical data. Here's how it works:\n",
     "\n",
     "1. **Input**: Real medical case chunks from our IRIS database\n",
     "2. **Process**: DeepEval's Synthesizer uses GPT-4 to create questions that could realistically be asked about this data\n",
     "3. **Output**: Question-answer pairs with expected responses\n",
     "\n",
     "**Why is this useful?**\n",
     "- Creates test cases that match your actual data\n",
     "- Saves time compared to manual test creation\n",
     "- Generates diverse question types you might not think of\n",
     "- Ensures evaluation covers real scenarios your users will encounter\n",
     "\n",
     "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to generate synthetic test cases using DeepEval's Synthesizer\n",
    "synthetic_test_cases = []\n",
    "\n",
    "if chunked_documents and os.getenv(\"OPENAI_API_KEY\"):\n",
    "    try:\n",
    "        print(\"\\nðŸ“Š STEP 2: Preparing our medical data for AI analysis...\\n   â†’ We have {len(chunked_documents)} chunks of medical case data\")\n",
    "        \n",
    "        # Initialize the DeepEval synthesizer\n",
"        print(\"   â†’ AI synthesizer ready (powered by GPT-4)\")\n",
    "        synthesizer = Synthesizer()\n",
    "        \n",
    "        # Use first 3 chunks for synthesis (to manage API costs)\n",
"        print(f\"   â†’ Selected {len(contexts_for_synthesis)} chunks for test generation (to manage costs)\")\n",
     "        \n",
     "        print(\"\\nðŸ§  STEP 3: AI is analyzing medical data and creating test questions...\")\n",
     "        print(\"   (This may take 10-30 seconds as GPT-4 reads and understands the medical content)\")\n",
    "        contexts_for_synthesis = [[doc.page_content] for doc in chunked_documents[:3]]\n",
    "        \n",
    "        # Generate synthetic test cases using correct format: List[List[str]]\n",
    "        synthetic_test_cases = synthesizer.generate_goldens_from_contexts(\n",
    "            contexts=contexts_for_synthesis,  # List of lists of strings\n",
    "            max_goldens_per_context=1  # Generate 1 test case per context\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… SUCCESS: Generated {len(synthetic_test_cases)} synthetic test cases!\")\n",
    "        print(\"   â†’ Each test case contains: Question + Expected Answer + Source Context\")\n",
    "        \n",
    "        # ðŸ“‹ STEP 4: Show what the AI created\n",
    "        if synthetic_test_cases:\n",
    "            print(\"\\nðŸ“‹ STEP 4: Let's examine what the AI generated...\")\n",
    "            sample = synthetic_test_cases[0]\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"ðŸ” EXAMPLE SYNTHETIC TEST CASE:\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"â“ QUESTION: {sample.input}\")\n",
    "            print(f\"\\nðŸ’¡ EXPECTED ANSWER: {sample.expected_output}\")\n",
    "            print(f\"\\nðŸ“„ SOURCE CONTEXT: {sample.context[:150]}...\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"\\nðŸ’­ Notice how the AI:\")\n",
    "            print(\"   â€¢ Created a realistic medical question from the data\")\n",
    "            print(\"   â€¢ Generated an appropriate expected answer\")\n",
    "            print(\"   â€¢ Linked it to the specific source context\")\n",
    "            print(\"   â€¢ This mimics real user questions about medical cases!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating synthetic test cases: {e}\")\n",
    "        print(\"This might be due to API rate limits. Using manual test cases instead.\")\n",
    "        synthetic_test_cases = []\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping synthetic generation (no chunked data or API key). Using manual test cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c: Combine Test Cases\n",
     "\n",
     "Now we'll combine our AI-generated test cases with manually created ones. This gives us:\n",
     "- **Synthetic cases**: AI-generated from real data (realistic scenarios)\n",
     "- **Manual cases**: Human-crafted (edge cases and specific scenarios)\n",
     "- **Best of both**: Comprehensive test coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”„ STEP 1: Create our master test suite\n",
    "print(\"ðŸ”„ Combining AI-generated and manual test cases...\")\n",
    "all_test_cases = []\n",
    "\n",
    "# Add synthetic test cases if available\n",
    "if synthetic_test_cases:\n",
    "    print(f\"   â†’ Adding {len(synthetic_test_cases)} AI-generated test cases\")\n",
    "    for case in synthetic_test_cases:\n",
    "        all_test_cases.append({\n",
    "            \"input\": case.input,\n",
    "            \"expected_output\": case.expected_output,\n",
    "            \"source\": \"synthetic\"  # Mark as AI-generated\n",
    "        })\n",
    "else:\n",
    "    print(\"   â†’ No synthetic test cases available (using manual only)\")\n",
    "\n",
    "# ðŸ“ STEP 2: Add manually crafted test cases\n",
    "print(\"\\nðŸ“ Adding manually crafted test cases...\")\n",
    "manual_test_cases = [\n",
    "    {\n",
    "        \"input\": \"What are common symptoms of knee problems in young patients?\",\n",
    "        \"expected_output\": \"Common symptoms include pain, swelling, limited range of motion, and difficulty with weight-bearing activities.\",\n",
    "        \"source\": \"manual\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How are fractures typically treated in elderly patients?\",\n",
    "        \"expected_output\": \"Treatment often involves surgical fixation, pain management, and careful consideration of the patient's overall health status.\",\n",
    "        \"source\": \"manual\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What diagnostic methods are used for abdominal pain?\",\n",
    "        \"expected_output\": \"Common diagnostic methods include physical examination, CT scans, ultrasound, and laboratory tests.\",\n",
    "        \"source\": \"manual\"\n",
    "    }\n",
    "]\n",
    "\n",
    "all_test_cases.extend(manual_test_cases)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Total test cases: {len(all_test_cases)}\")\n",
    "synthetic_count = len([tc for tc in all_test_cases if tc['source'] == 'synthetic'])\n",
    "manual_count = len([tc for tc in all_test_cases if tc['source'] == 'manual'])\n",
    "print(f\"  - Synthetic: {synthetic_count}\")\n",
    "print(f\"  - Manual: {manual_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run RAG Pipeline on Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG pipeline on test cases\n",
    "evaluation_results = []\n",
    "\n",
    "for i, test_case in enumerate(all_test_cases):\n",
    "    print(f\"Processing test case {i+1}/{len(all_test_cases)}...\")\n",
    "    \n",
    "    question = test_case[\"input\"]\n",
    "    expected_answer = test_case[\"expected_output\"]\n",
    "    \n",
    "    try:\n",
    "        actual_answer, retrieved_contexts = workshop_chat_rag_pipeline(question, db, llm)\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            \"question\": question,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"retrieved_contexts\": retrieved_contexts\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing test case {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully processed {len(evaluation_results)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate with DeepEval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepEval metrics\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "faithfulness_metric = FaithfulnessMetric(threshold=0.7)\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(threshold=0.7)\n",
    "contextual_recall_metric = ContextualRecallMetric(threshold=0.7)\n",
    "\n",
    "# Create LLMTestCase objects for DeepEval\n",
    "test_cases_for_evaluation = []\n",
    "\n",
    "for result in evaluation_results:\n",
    "    test_case = LLMTestCase(\n",
    "        input=result[\"question\"],\n",
    "        actual_output=result[\"actual_answer\"],\n",
    "        expected_output=result[\"expected_answer\"],\n",
    "        retrieval_context=result[\"retrieved_contexts\"]\n",
    "    )\n",
    "    test_cases_for_evaluation.append(test_case)\n",
    "\n",
    "print(f\"Created {len(test_cases_for_evaluation)} test cases for DeepEval evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with DeepEval\n",
    "print(\"Running DeepEval evaluation...\")\n",
    "\n",
    "try:\n",
    "    evaluation_scores = {\n",
    "        \"answer_relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contextual_relevancy\": [],\n",
    "        \"contextual_recall\": []\n",
    "    }\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases_for_evaluation):\n",
    "        print(f\"Evaluating test case {i+1}/{len(test_cases_for_evaluation)}...\")\n",
    "        \n",
    "        # Evaluate each metric\n",
    "        answer_relevancy_metric.measure(test_case)\n",
    "        evaluation_scores[\"answer_relevancy\"].append(answer_relevancy_metric.score)\n",
    "        \n",
    "        faithfulness_metric.measure(test_case)\n",
    "        evaluation_scores[\"faithfulness\"].append(faithfulness_metric.score)\n",
    "        \n",
    "        contextual_relevancy_metric.measure(test_case)\n",
    "        evaluation_scores[\"contextual_relevancy\"].append(contextual_relevancy_metric.score)\n",
    "        \n",
    "        contextual_recall_metric.measure(test_case)\n",
    "        evaluation_scores[\"contextual_recall\"].append(contextual_recall_metric.score)\n",
    "    \n",
    "    print(\"âœ… Evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during evaluation: {e}\")\n",
    "    # Create dummy scores for demonstration\n",
    "    evaluation_scores = {\n",
    "        \"answer_relevancy\": [0.8, 0.7, 0.9],\n",
    "        \"faithfulness\": [0.85, 0.75, 0.8],\n",
    "        \"contextual_relevancy\": [0.7, 0.8, 0.85],\n",
    "        \"contextual_recall\": [0.75, 0.7, 0.8]\n",
    "    }\n",
    "    print(\"Using dummy scores for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Analyze and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate average scores\n",
    "avg_scores = {}\n",
    "for metric, scores in evaluation_scores.items():\n",
    "    avg_scores[metric] = np.mean(scores) if scores else 0\n",
    "\n",
    "print(\"ðŸ“Š RAG System Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, avg_score in avg_scores.items():\n",
    "    print(f\"{metric.replace('_', ' ').title()}: {avg_score:.3f}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('RAG System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "metrics = list(avg_scores.keys())\n",
    "scores = list(avg_scores.values())\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "bars = ax1.bar(metrics, scores, color=colors)\n",
    "ax1.set_title('Average Evaluation Scores')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)\n",
    "scores_radar = list(avg_scores.values())\n",
    "scores_radar += scores_radar[:1]\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "ax2.plot(angles, scores_radar, 'o-', linewidth=2, color='#FF6B6B')\n",
    "ax2.fill(angles, scores_radar, alpha=0.25, color='#FF6B6B')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('RAG Performance Radar')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"\\nðŸ” Performance Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "best_metric = max(avg_scores, key=avg_scores.get)\n",
    "worst_metric = min(avg_scores, key=avg_scores.get)\n",
    "overall_avg = np.mean(list(avg_scores.values()))\n",
    "\n",
    "print(f\"ðŸŽ¯ Best Performing Metric: {best_metric.replace('_', ' ').title()}\")\n",
    "print(f\"   Score: {avg_scores[best_metric]:.3f}\")\n",
    "print(f\"\\nðŸ”§ Needs Improvement: {worst_metric.replace('_', ' ').title()}\")\n",
    "print(f\"   Score: {avg_scores[worst_metric]:.3f}\")\n",
    "print(f\"\\nðŸ“Š Overall Average: {overall_avg:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Improvement Recommendations:\")\n",
    "print(\"â€¢ Scores > 0.8: Excellent performance\")\n",
    "print(\"â€¢ Scores 0.7-0.8: Good performance\")\n",
    "print(\"â€¢ Scores < 0.7: Needs improvement\")\n",
    "\n",
    "if avg_scores['answer_relevancy'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Answer Relevancy Tips:\")\n",
    "    print(\"  - Improve prompt engineering\")\n",
    "    print(\"  - Add question classification\")\n",
    "\n",
    "if avg_scores['faithfulness'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Faithfulness Tips:\")\n",
    "    print(\"  - Improve retrieval quality\")\n",
    "    print(\"  - Add explicit context adherence instructions\")\n",
    "\n",
    "if avg_scores['contextual_relevancy'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Contextual Relevancy Tips:\")\n",
    "    print(\"  - Optimize embedding model\")\n",
    "    print(\"  - Tune retrieval parameters\")\n",
    "\n",
    "if avg_scores['contextual_recall'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Contextual Recall Tips:\")\n",
    "    print(\"  - Increase number of retrieved documents\")\n",
    "    print(\"  - Improve document chunking strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This test-first framework using DeepEval provides:\n",
    "\n",
    "1. **Objective Measurement**: Quantitative metrics for RAG system performance\n",
    "2. **Systematic Improvement**: Data-driven insights for optimization\n",
    "3. **Regression Detection**: Ability to catch performance degradation\n",
    "4. **Comparative Analysis**: Framework for comparing different approaches\n",
    "\n",
    "Use this evaluation framework throughout your RAG development process to ensure consistent quality and continuous improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
