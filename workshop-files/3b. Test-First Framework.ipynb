{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Test-First Framework for RAG Evaluation\n",
    "\n",
    "In this notebook, we'll implement a comprehensive evaluation framework using **DeepEval**, a modern evaluation library specifically designed for LLM applications and RAG systems.\n",
    "\n",
    "### Why DeepEval for RAG Evaluation?\n",
    "\n",
    "DeepEval provides several advantages:\n",
    "1. **RAG-Specific Metrics**: Built-in metrics for answer relevancy, faithfulness, and contextual recall\n",
    "2. **Synthetic Data Generation**: Automatically generate test cases from your knowledge base\n",
    "3. **LLM-as-a-Judge**: Uses advanced LLMs to evaluate responses intelligently\n",
    "4. **Easy Integration**: Simple API that works well with existing RAG pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# DeepEval imports\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    ContextualRecallMetric\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "# Langchain imports for our RAG system\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_iris import IRISVector\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Database connection details\n",
    "username = 'SuperUser'\n",
    "password = 'SYS'\n",
    "hostname = 'localhost'\n",
    "port = 1972\n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "COLLECTION_NAME = \"case_reports\"\n",
    "\n",
    "# Initialize components\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "db = IRISVector(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING\n",
    ")\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "print(f\"Retriever initialized: {retriever}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and check API key\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âš ï¸ Warning: OPENAI_API_KEY not found. Please set your OpenAI API key.\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key found. DeepEval is ready to use.\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Simple RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_pipeline(question: str, retriever, llm) -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Simple RAG pipeline that retrieves relevant documents and generates an answer.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    contexts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    # Create prompt and generate answer\n",
    "    context_text = \"\\n\\n\".join(contexts)\n",
    "    prompt = f\"\"\"\n",
    "Based on the following medical case reports, answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content, contexts\n",
    "\n",
    "# Test the pipeline\n",
    "test_question = \"What are common symptoms of knee problems in young patients?\"\n",
    "test_answer, test_contexts = simple_rag_pipeline(test_question, retriever, llm)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {test_answer}\")\n",
    "print(f\"Retrieved {len(test_contexts)} contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manual test cases for evaluation\n",
    "manual_test_cases = [\n",
    "    {\n",
    "        \"input\": \"What are common symptoms of knee problems in young patients?\",\n",
    "        \"expected_output\": \"Common symptoms include pain, swelling, limited range of motion, and difficulty with weight-bearing activities.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"How are fractures typically treated in elderly patients?\",\n",
    "        \"expected_output\": \"Treatment often involves surgical fixation, pain management, and careful consideration of the patient's overall health status.\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What diagnostic methods are used for abdominal pain?\",\n",
    "        \"expected_output\": \"Common diagnostic methods include physical examination, CT scans, ultrasound, and laboratory tests.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(manual_test_cases)} test cases for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run RAG Pipeline on Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG pipeline on test cases\n",
    "evaluation_results = []\n",
    "\n",
    "for i, test_case in enumerate(manual_test_cases):\n",
    "    print(f\"Processing test case {i+1}/{len(manual_test_cases)}...\")\n",
    "    \n",
    "    question = test_case[\"input\"]\n",
    "    expected_answer = test_case[\"expected_output\"]\n",
    "    \n",
    "    try:\n",
    "        actual_answer, retrieved_contexts = simple_rag_pipeline(question, retriever, llm)\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            \"question\": question,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"retrieved_contexts\": retrieved_contexts\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing test case {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully processed {len(evaluation_results)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate with DeepEval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepEval metrics\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "faithfulness_metric = FaithfulnessMetric(threshold=0.7)\n",
    "contextual_relevancy_metric = ContextualRelevancyMetric(threshold=0.7)\n",
    "contextual_recall_metric = ContextualRecallMetric(threshold=0.7)\n",
    "\n",
    "# Create LLMTestCase objects for DeepEval\n",
    "test_cases_for_evaluation = []\n",
    "\n",
    "for result in evaluation_results:\n",
    "    test_case = LLMTestCase(\n",
    "        input=result[\"question\"],\n",
    "        actual_output=result[\"actual_answer\"],\n",
    "        expected_output=result[\"expected_answer\"],\n",
    "        retrieval_context=result[\"retrieved_contexts\"]\n",
    "    )\n",
    "    test_cases_for_evaluation.append(test_case)\n",
    "\n",
    "print(f\"Created {len(test_cases_for_evaluation)} test cases for DeepEval evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with DeepEval\n",
    "print(\"Running DeepEval evaluation...\")\n",
    "\n",
    "try:\n",
    "    evaluation_scores = {\n",
    "        \"answer_relevancy\": [],\n",
    "        \"faithfulness\": [],\n",
    "        \"contextual_relevancy\": [],\n",
    "        \"contextual_recall\": []\n",
    "    }\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases_for_evaluation):\n",
    "        print(f\"Evaluating test case {i+1}/{len(test_cases_for_evaluation)}...\")\n",
    "        \n",
    "        # Evaluate each metric\n",
    "        answer_relevancy_metric.measure(test_case)\n",
    "        evaluation_scores[\"answer_relevancy\"].append(answer_relevancy_metric.score)\n",
    "        \n",
    "        faithfulness_metric.measure(test_case)\n",
    "        evaluation_scores[\"faithfulness\"].append(faithfulness_metric.score)\n",
    "        \n",
    "        contextual_relevancy_metric.measure(test_case)\n",
    "        evaluation_scores[\"contextual_relevancy\"].append(contextual_relevancy_metric.score)\n",
    "        \n",
    "        contextual_recall_metric.measure(test_case)\n",
    "        evaluation_scores[\"contextual_recall\"].append(contextual_recall_metric.score)\n",
    "    \n",
    "    print(\"âœ… Evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during evaluation: {e}\")\n",
    "    # Create dummy scores for demonstration\n",
    "    evaluation_scores = {\n",
    "        \"answer_relevancy\": [0.8, 0.7, 0.9],\n",
    "        \"faithfulness\": [0.85, 0.75, 0.8],\n",
    "        \"contextual_relevancy\": [0.7, 0.8, 0.85],\n",
    "        \"contextual_recall\": [0.75, 0.7, 0.8]\n",
    "    }\n",
    "    print(\"Using dummy scores for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Analyze and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate average scores\n",
    "avg_scores = {}\n",
    "for metric, scores in evaluation_scores.items():\n",
    "    avg_scores[metric] = np.mean(scores) if scores else 0\n",
    "\n",
    "print(\"ðŸ“Š RAG System Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, avg_score in avg_scores.items():\n",
    "    print(f\"{metric.replace('_', ' ').title()}: {avg_score:.3f}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('RAG System Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "metrics = list(avg_scores.keys())\n",
    "scores = list(avg_scores.values())\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "\n",
    "bars = ax1.bar(metrics, scores, color=colors)\n",
    "ax1.set_title('Average Evaluation Scores')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)\n",
    "scores_radar = list(avg_scores.values())\n",
    "scores_radar += scores_radar[:1]\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "ax2.plot(angles, scores_radar, 'o-', linewidth=2, color='#FF6B6B')\n",
    "ax2.fill(angles, scores_radar, alpha=0.25, color='#FF6B6B')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels([m.replace('_', ' ').title() for m in metrics])\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('RAG Performance Radar')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"\\nðŸ” Performance Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "best_metric = max(avg_scores, key=avg_scores.get)\n",
    "worst_metric = min(avg_scores, key=avg_scores.get)\n",
    "overall_avg = np.mean(list(avg_scores.values()))\n",
    "\n",
    "print(f\"ðŸŽ¯ Best Performing Metric: {best_metric.replace('_', ' ').title()}\")\n",
    "print(f\"   Score: {avg_scores[best_metric]:.3f}\")\n",
    "print(f\"\\nðŸ”§ Needs Improvement: {worst_metric.replace('_', ' ').title()}\")\n",
    "print(f\"   Score: {avg_scores[worst_metric]:.3f}\")\n",
    "print(f\"\\nðŸ“Š Overall Average: {overall_avg:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Improvement Recommendations:\")\n",
    "print(\"â€¢ Scores > 0.8: Excellent performance\")\n",
    "print(\"â€¢ Scores 0.7-0.8: Good performance\")\n",
    "print(\"â€¢ Scores < 0.7: Needs improvement\")\n",
    "\n",
    "if avg_scores['answer_relevancy'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Answer Relevancy Tips:\")\n",
    "    print(\"  - Improve prompt engineering\")\n",
    "    print(\"  - Add question classification\")\n",
    "\n",
    "if avg_scores['faithfulness'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Faithfulness Tips:\")\n",
    "    print(\"  - Improve retrieval quality\")\n",
    "    print(\"  - Add explicit context adherence instructions\")\n",
    "\n",
    "if avg_scores['contextual_relevancy'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Contextual Relevancy Tips:\")\n",
    "    print(\"  - Optimize embedding model\")\n",
    "    print(\"  - Tune retrieval parameters\")\n",
    "\n",
    "if avg_scores['contextual_recall'] < 0.7:\n",
    "    print(\"\\nðŸ”§ Contextual Recall Tips:\")\n",
    "    print(\"  - Increase number of retrieved documents\")\n",
    "    print(\"  - Improve document chunking strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This test-first framework using DeepEval provides:\n",
    "\n",
    "1. **Objective Measurement**: Quantitative metrics for RAG system performance\n",
    "2. **Systematic Improvement**: Data-driven insights for optimization\n",
    "3. **Regression Detection**: Ability to catch performance degradation\n",
    "4. **Comparative Analysis**: Framework for comparing different approaches\n",
    "\n",
    "Use this evaluation framework throughout your RAG development process to ensure consistent quality and continuous improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ready25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
