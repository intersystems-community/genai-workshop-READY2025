{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test-First Framework for RAG Evaluation\n",
    "\n",
    "In the previous sections, we explored how to load data, create vector embeddings, and use helper libraries like Langchain to build the foundational components of a Retrieval Augmented Generation (RAG) system. We successfully set up a retriever capable of fetching relevant documents from our InterSystems IRIS database.\n",
    "\n",
    "However, building a RAG system is an iterative process. How do we know if our retriever is fetching the *most* relevant documents? How do we measure the quality of the answers generated by our system? This is where evaluation comes in.\n",
    "\n",
    "In this notebook, we will introduce the concept of a **test-first framework** for RAG systems. This approach emphasizes creating evaluation mechanisms *before* or *alongside* development, allowing us to continuously measure and improve our system's performance. We will leverage **Ragas**, a powerful open-source library specifically designed for evaluating RAG pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives:\n",
    "1. Understand the importance of evaluation in RAG systems.\n",
    "2. Learn about the test-first methodology for iterative improvement.\n",
    "3. Introduce Ragas and its capabilities for RAG evaluation.\n",
    "4. Generate a synthetic test dataset using Ragas.\n",
    "5. Implement and understand key Ragas evaluation metrics (e.g., faithfulness, answer relevancy, context precision, context recall).\n",
    "6. Evaluate our current RAG setup using the generated dataset and Ragas.\n",
    "7. Discuss how to interpret evaluation results to guide improvements.\n",
    "8. ?? actually improve something based on first results.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up the Environment and Dependencies\n",
    "\n",
    "First, let's install Ragas and ensure our environment is ready. We'll also need to import libraries from the previous notebook to re-establish our connection to IRIS and our document retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ragas==0.1.7 pandas langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables (ensure your .env file has OPENAI_API_KEY)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the necessary Langchain and IRIS components, similar to Notebook 3, to access our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings # Using FastEmbed as in Notebook 3\n",
    "from langchain_iris import IRISVector\n",
    "from langchain_openai import ChatOpenAI # For Ragas generator and evaluation\n",
    "\n",
    "# Database connection details (same as Notebook 3)\n",
    "username = '_SYSTEM'\n",
    "password = 'SYsysS'\n",
    "hostname = 'localhost' # Assuming IRIS is running locally in the workshop environment\n",
    "port = 1972\n",
    "namespace = 'USER'\n",
    "CONNECTION_STRING = f\"iris://{username}:{password}@{hostname}:{port}/{namespace}\"\n",
    "\n",
    "COLLECTION_NAME = \"case_reports\" # Same collection as Notebook 3\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "\n",
    "# Initialize IRISVector store\n",
    "db = IRISVector(\n",
    "    embedding_function=embeddings,  # â† Change \"embedding\" to \"embedding_function\"\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING\n",
    ")\n",
    "\n",
    "# Get the retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "print(f\"Retriever initialized: {retriever}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the documents that were loaded in Notebook 3 to generate our test set. If you haven't run Notebook 3 to populate the `case_reports` collection and create chunks, you might need to adapt the following or ensure that data exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents again to be used by Ragas TestsetGenerator\n",
    "# This assumes the data loading and chunking process from Notebook 3 has been performed\n",
    "# or that the chunks are accessible/recreatable.\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='./data/healthcare/augmented_notes_100.jsonl',\n",
    "    jq_schema='.note',\n",
    "    json_lines=True\n",
    ")\n",
    "documents_for_testgen = loader.load() # These are whole documents\n",
    "\n",
    "# We need the actual text content for the generator\n",
    "# Ragas testset generator works well with Document objects from Langchain\n",
    "print(f\"Loaded {len(documents_for_testgen)} documents for test set generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Understanding RAG Evaluation & The Test-First Mindset\n",
    "\n",
    "A RAG system has two main parts: **Retrieval** and **Generation**.\n",
    "- **Retrieval**: How well does our system find relevant information from the knowledge base (our IRIS database)?\n",
    "- **Generation**: Given the retrieved information, how well does our system synthesize a coherent, accurate, and relevant answer?\n",
    "\n",
    "**Why evaluate?**\n",
    "- **Identify Weaknesses**: Pinpoint whether issues lie in retrieval, generation, or both.\n",
    "- **Measure Progress**: Quantify improvements as we tune parameters (e.g., chunk size, embedding models, prompts).\n",
    "- **Prevent Hallucinations**: Ensure generated answers are grounded in the provided context.\n",
    "- **Ensure Relevance**: Verify that answers directly address the user's query.\n",
    "\n",
    "The **Test-First Mindset** encourages us to:\n",
    "1. **Define Success**: What does a \"good\" RAG response look like for our use case?\n",
    "2. **Establish Baselines**: Measure the performance of our initial RAG setup.\n",
    "3. **Iterate and Measure**: As we make changes (e.g., try different embedding models, adjust chunking strategies, refine prompts), re-evaluate to see if performance improves.\n",
    "\n",
    "This iterative loop of `Develop -> Test -> Analyze -> Refine` is key to building robust and reliable RAG applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Introducing Ragas\n",
    "\n",
    "[Ragas](https://docs.ragas.io/) is a framework that helps you evaluate your RAG pipelines. It provides a set of metrics tailored for RAG systems and can even help generate synthetic test data.\n",
    "\n",
    "**Key Ragas Metrics we'll explore:**\n",
    "\n",
    "**Retrieval-focused:**\n",
    "- `context_precision`: Measures the signal-to-noise ratio of the retrieved contexts. Answers: *Are all the retrieved contexts relevant?*\n",
    "- `context_recall`: Measures the ability of the retriever to retrieve all necessary information needed to answer the question. Answers: *Did we retrieve all the relevant contexts?*\n",
    "\n",
    "**Generation-focused (conditioned on retrieved context):**\n",
    "- `faithfulness`: Measures the factual consistency of the generated answer against the given context. Answers: *Is the answer grounded in the provided context, or is it hallucinating?*\n",
    "- `answer_relevancy`: Measures how relevant the generated answer is to the input question. Answers: *Does the answer directly address the question?*\n",
    "\n",
    "Some metrics, like `faithfulness` and `answer_relevancy`, require an LLM (e.g., GPT-3.5/4) to perform the evaluation, as they assess semantic qualities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generating a Synthetic Test Dataset with Ragas\n",
    "\n",
    "To evaluate our RAG system, we need a test dataset consisting of questions, ground truth answers, and the contexts that should ideally be retrieved. Ragas can help us generate such a dataset from our existing documents.\n",
    "\n",
    "The `TestsetGenerator` from Ragas uses an LLM (generator_llm) and a critic LLM (critic_llm) along with an embedding model to create question-context-answer triplets from your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings as RagasOpenAIEmbeddings # Ragas works well with OpenAI embeddings for generation\n",
    "\n",
    "# Initialize the LLMs and Embeddings for the TestsetGenerator\n",
    "# Note: Testset generation can be resource-intensive and may take time.\n",
    "# For the workshop, we might use a small subset of documents or a pre-generated set if time is a constraint.\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\") # or gpt-4 if available and budget allows\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4-turbo-preview\") # Critic often benefits from a stronger model\n",
    "# Ragas testset generator uses its own embeddings, often OpenAI for consistency in generation quality\n",
    "ragas_embeddings = RagasOpenAIEmbeddings()\n",
    "\n",
    "test_generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings=ragas_embeddings\n",
    ")\n",
    "\n",
    "# Let's generate a small test set from a few documents to see how it works.\n",
    "# distributions: how many questions of each type to generate\n",
    "# For a real scenario, you'd use more documents and generate a larger test set.\n",
    "num_documents_for_test_generation = 5 # Adjust as needed for speed\n",
    "if len(documents_for_testgen) > num_documents_for_test_generation:\n",
    "    sample_documents_for_testgen = documents_for_testgen[:num_documents_for_test_generation]\n",
    "else:\n",
    "    sample_documents_for_testgen = documents_for_testgen\n",
    "\n",
    "print(f\"Generating testset from {len(sample_documents_for_testgen)} documents...\")\n",
    "\n",
    "# This step can take a while and will make calls to the OpenAI API.\n",
    "try:\n",
    "    testset = test_generator.generate_with_langchain_docs(\n",
    "        documents=sample_documents_for_testgen, \n",
    "        test_size=10, # Number of question/answer pairs to generate\n",
    "        distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "    )\n",
    "    print(\"Testset generation complete.\")\n",
    "    test_df = testset.to_pandas()\n",
    "    print(test_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error during testset generation: {e}\")\n",
    "    print(\"This might be due to API limits, network issues, or document content.\")\n",
    "    print(\"For the workshop, we might proceed with a placeholder or pre-generated dataset if this fails.\")\n",
    "    # Create a dummy dataframe for workshop continuation if generation fails\n",
    "    test_df = pd.DataFrame({\n",
    "        'question': ['What is the primary symptom of condition X?', 'How is Y treated?'],\n",
    "        'contexts': [['Relevant context for X...'], ['Relevant context for Y...']],\n",
    "        'ground_truth': ['The primary symptom is Z.', 'Y is treated with A and B.'],\n",
    "        'evolution_type': ['simple', 'simple']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated `test_df` DataFrame typically contains columns like:\n",
    "- `question`: The synthetically generated question.\n",
    "- `contexts`: The chunk(s) of text from your documents that are relevant to the question.\n",
    "- `ground_truth`: The synthetically generated 'ideal' answer to the question, based on the contexts.\n",
    "- `evolution_type`: The method used to generate the question (e.g., simple, reasoning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluating the RAG System with Ragas\n",
    "\n",
    "Now that we have a test set (even if it's a small or dummy one for now), we can evaluate our RAG system. To do this with Ragas, we need to define how our RAG system retrieves contexts and generates answers.\n",
    "\n",
    "Ragas `evaluate` function expects the test data in a specific format (typically a Hugging Face dataset or a pandas DataFrame with 'question', 'contexts', 'answer', 'ground_truth' columns). Our retriever will provide the 'contexts' and our generator (LLM) will provide the 'answer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import asyncio # Ragas evaluation can be async\n",
    "\n",
    "# Define our RAG chain components for Ragas\n",
    "# 1. Retriever: We already have `retriever` from IRISVector\n",
    "# 2. Generator: An LLM that takes a question and context to produce an answer.\n",
    "\n",
    "llm_for_ragas_eval = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# We need to prepare our data for the Ragas evaluate function.\n",
    "# It expects 'question', 'ground_truth', and then it will use our retriever and generator\n",
    "# to get 'contexts' and 'answer'.\n",
    "\n",
    "# Let's adapt our test_df. Ragas expects 'ground_truth' for some metrics.\n",
    "# The 'contexts' in test_df are the ideal contexts used to generate ground_truth.\n",
    "# For evaluation, Ragas will call *our* retriever to get the actual retrieved 'contexts'.\n",
    "\n",
    "if 'test_df' not in locals() or test_df.empty:\n",
    "    print(\"Test DataFrame is not available. Skipping evaluation.\")\n",
    "else:\n",
    "    questions = test_df[\"question\"].tolist()\n",
    "    ground_truths = test_df[\"ground_truth\"].tolist()\n",
    "\n",
    "    # Simulate RAG pipeline to get answers and retrieved contexts\n",
    "    answers = []\n",
    "    retrieved_contexts_list = []\n",
    "\n",
    "    for question in questions:\n",
    "        # 1. Retrieve contexts using our IRISVector retriever\n",
    "        retrieved_docs = retriever.get_relevant_documents(question)\n",
    "        retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
    "        retrieved_contexts_list.append(retrieved_contexts)\n",
    "        \n",
    "        # 2. Generate answer using LLM with retrieved_contexts\n",
    "        context_str = \"\\n\".join(retrieved_contexts)\n",
    "        prompt = f\"Question: {question}\\nContext:\\n{context_str}\\nAnswer:\"\n",
    "        response = llm_for_ragas_eval.invoke(prompt)\n",
    "        answers.append(response.content)\n",
    "\n",
    "    # Create a Hugging Face Dataset for Ragas\n",
    "    eval_data = {\n",
    "        'question': questions,\n",
    "        'answer': answers,\n",
    "        'contexts': retrieved_contexts_list,\n",
    "        'ground_truth': ground_truths # from the generated testset\n",
    "    }\n",
    "    dataset = Dataset.from_dict(eval_data)\n",
    "\n",
    "    print(\"Dataset prepared for Ragas evaluation:\")\n",
    "    print(dataset)\n",
    "\n",
    "    # Define metrics for evaluation\n",
    "    metrics = [\n",
    "        faithfulness,  # Requires 'question', 'answer', 'contexts'\n",
    "        answer_relevancy, # Requires 'question', 'answer', 'contexts'\n",
    "        context_precision, # Requires 'question', 'ground_truth', 'contexts'\n",
    "        context_recall,    # Requires 'question', 'ground_truth', 'contexts'\n",
    "    ]\n",
    "\n",
    "    print(\"\\nStarting Ragas evaluation...\")\n",
    "    # Note: This will make LLM calls for metrics like faithfulness and answer_relevancy.\n",
    "    # Ensure your OpenAI API key is set up and has sufficient quota.\n",
    "    try:\n",
    "        # Ragas evaluation can be run asynchronously\n",
    "        # If running in a Jupyter notebook, you might need to handle the event loop\n",
    "        # For simplicity here, we try to run it, but in some envs `asyncio.run` might be needed\n",
    "        # or use `evaluate(..., is_async=False)` if available and suitable for the Ragas version\n",
    "        results = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=metrics,\n",
    "            llm=llm_for_ragas_eval, # LLM for metrics that need it\n",
    "            embeddings=ragas_embeddings # Embeddings for metrics that need it (e.g. answer_relevancy with embedding distance)\n",
    "            # is_async=False # Check Ragas documentation for synchronous execution if needed\n",
    "        )\n",
    "        print(\"Ragas evaluation complete.\")\n",
    "        results_df = results.to_pandas()\n",
    "        print(results_df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Ragas evaluation: {e}\")\n",
    "        print(\"This could be due to API calls, data format, or async issues in this environment.\")\n",
    "        results_df = pd.DataFrame() # Empty dataframe if evaluation fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualizing and Interpreting Results\n",
    "\n",
    "The `results_df` DataFrame contains the scores for each metric for every question-answer pair. We can calculate average scores to get an overall sense of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(\"Overall Average Scores:\")\n",
    "    # Calculate mean scores, ensuring we only average numeric columns (the metrics)\n",
    "    metric_columns = [m.name for m in metrics] # Get metric names\n",
    "    average_scores = results_df[metric_columns].mean().reset_index()\n",
    "    average_scores.columns = ['Metric', 'Average Score']\n",
    "    print(average_scores)\n",
    "\n",
    "    # Plotting the average scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Average Score', y='Metric', data=average_scores, palette='viridis')\n",
    "    plt.title('Average RAGS Evaluation Scores')\n",
    "    plt.xlabel('Average Score (0 to 1)')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.xlim(0, 1) # Scores are typically between 0 and 1\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No evaluation results to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the Scores:**\n",
    "\n",
    "- **High `faithfulness` (e.g., >0.8):** Good. The answers are factually consistent with the retrieved context.\n",
    "- **Low `faithfulness`:** Bad. The model might be hallucinating or making up information not present in the context. *Potential Fixes: Improve prompts, use a more capable LLM for generation, ensure retrieved context is sufficient.*\n",
    "\n",
    "- **High `answer_relevancy` (e.g., >0.8):** Good. The answers are relevant to the questions.\n",
    "- **Low `answer_relevancy`:** Bad. The answers might be off-topic or not address the user's intent. *Potential Fixes: Better prompts, ensure retrieved context is highly relevant to the question.*\n",
    "\n",
    "- **High `context_precision` (e.g., >0.8):** Good. Most of the retrieved context is relevant to the question.\n",
    "- **Low `context_precision`:** Bad. The retriever is fetching a lot of irrelevant information, which can confuse the generator. *Potential Fixes: Tune retriever (e.g., `k` value for number of docs), improve chunking, use better embedding model, refine search query formulation.*\n",
    "\n",
    "- **High `context_recall` (e.g., >0.8):** Good. The retriever is finding all the necessary pieces of information from the knowledge base.\n",
    "- **Low `context_recall`:** Bad. The retriever is missing important information, leading to incomplete answers. *Potential Fixes: Improve document coverage, better chunking strategy, ensure all relevant information is indexed, use different retrieval strategies (e.g., hybrid search).*\n",
    "\n",
    "The ideal scores depend on the specific application. For critical applications, you'd aim for very high scores across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Using Evaluation Results for Improvement (The Test-First Loop)\n",
    "\n",
    "This is where the test-first framework shines. The evaluation results are not just a report card; they are a diagnostic tool.\n",
    "\n",
    "**Example Iteration Cycle:**\n",
    "\n",
    "1.  **Analyze Results:** Suppose our initial `context_recall` is low (e.g., 0.6). This means our retriever isn't finding all the relevant information.\n",
    "2.  **Formulate Hypothesis:** Perhaps our document chunks are too large, causing specific details to be buried. Or maybe the embedding model isn't capturing the nuances of our domain well.\n",
    "3.  **Implement Change:** \n",
    "    *   Try a smaller `chunk_size` in `RecursiveCharacterTextSplitter` (as explored in Notebook 3).\n",
    "    *   Experiment with a different embedding model (e.g., switch from `FastEmbedEmbeddings` to `OpenAIEmbeddings` or a domain-specific Hugging Face model for the main RAG pipeline, not just Ragas eval).\n",
    "4.  **Re-Evaluate:** Run the Ragas evaluation again with the *exact same test set*.\n",
    "5.  **Compare:** Did `context_recall` improve? Did other metrics change (sometimes improving one metric can slightly degrade another)?\n",
    "\n",
    "This iterative process of `Evaluate -> Hypothesize -> Change -> Re-evaluate` is central to systematically improving your RAG system. In the upcoming notebooks (like Notebook 4: Connecting Chat to Vectors and beyond), we will be making changes to our RAG pipeline. This evaluation framework will be invaluable for measuring the impact of those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've laid the groundwork for a test-first approach to RAG development by:\n",
    "- Understanding the critical role of evaluation.\n",
    "- Introducing Ragas as a powerful tool for RAG assessment.\n",
    "- Generating a synthetic test dataset.\n",
    "- Implementing key Ragas metrics to evaluate our retriever (from IRISVector) and a generator.\n",
    "\n",
    "This framework provides us with a quantitative way to measure the performance of our RAG system. As we proceed to build more sophisticated chat applications and tune our retrieval and generation strategies in subsequent notebooks, we can continuously refer back to these evaluation techniques to guide our development and ensure we are making tangible improvements.\n",
    "\n",
    "**Next:** In Notebook 4, \"Connecting Chat to Vectors,\" we will focus on building a more interactive chat application. The evaluation methods learned here will be crucial for assessing how well that chat application performs in terms of retrieval accuracy and response quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
